{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d8763a",
   "metadata": {},
   "source": [
    "# ğŸ¥¬ Kalimati Tarkari Dataset: Fruits & Vegetables Price Analysis\n",
    "\n",
    "## ğŸ“Š About Dataset\n",
    "\n",
    "### **Kalimati Tarkari Dataset**\n",
    "This comprehensive dataset contains historical price data for fruits and vegetables from the **Kalimati Fruits and Vegetable Market Development Board**, Nepal's largest wholesale market for agricultural produce. The data has been meticulously scraped from the official website: [https://kalimatimarket.gov.np/](https://kalimatimarket.gov.np/)\n",
    "\n",
    "The dataset captures daily minimum, maximum, and average prices for a wide variety of commodities, providing valuable insights into market trends, seasonal variations, and price dynamics in Nepal's agricultural sector.\n",
    "\n",
    "ğŸ“¦ **Dataset Source**: [Kaggle - Kalimati Tarkari Dataset](https://www.kaggle.com/datasets/nischallal/kalimati-tarkari-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Context\n",
    "\n",
    "The Kalimati Market serves as the primary wholesale hub for fruits and vegetables in Nepal, directly influencing retail prices across the country. Understanding price patterns in this market is crucial for:\n",
    "\n",
    "- **Farmers** seeking optimal selling times for their produce\n",
    "- **Retailers** planning inventory and pricing strategies  \n",
    "- **Policymakers** monitoring food security and inflation\n",
    "- **Consumers** understanding seasonal price fluctuations\n",
    "- **Researchers** analyzing agricultural economics and supply chain dynamics\n",
    "\n",
    "This dataset represents years of daily price records, capturing the pulse of Nepal's agricultural market and offering a window into the economic realities of food distribution in South Asia.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Content\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "- **280,000+ records** spanning multiple years of daily price data\n",
    "- **Multiple commodity categories**: Vegetables (tomatoes, potatoes, leafy greens, etc.), Fruits (bananas, mangoes, apples, etc.), and specialty items\n",
    "- **Price metrics**: Minimum, Maximum, and Average prices per unit (Kg/Dozen/Piece)\n",
    "- **Temporal data**: Date-wise records enabling time series analysis\n",
    "- **Unit specifications**: Clear measurement units for each commodity\n",
    "\n",
    "### Key Features:\n",
    "- Daily price updates for 70+ different commodities\n",
    "- Seasonal variation patterns across different produce types\n",
    "- Price volatility indicators through min-max spreads\n",
    "- Historical trends for forecasting and predictive modeling\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ™ Acknowledgements\n",
    "\n",
    "We extend our sincere gratitude to:\n",
    "\n",
    "- **Kalimati Fruits and Vegetable Market Development Board** for maintaining transparent and accessible price records\n",
    "- The **Government of Nepal** for supporting agricultural market information systems\n",
    "- **Open data initiatives** that make agricultural market data publicly available for research and analysis\n",
    "\n",
    "This dataset would not be possible without the continuous efforts of market officials who diligently record and publish daily price information, contributing to market transparency and informed decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Inspiration & Research Questions\n",
    "\n",
    "This dataset opens doors to numerous analytical opportunities:\n",
    "\n",
    "### ğŸ“ˆ Time Series Analysis:\n",
    "- Can we predict future prices based on historical trends?\n",
    "- What are the seasonal patterns for different commodities?\n",
    "- How do prices fluctuate during festivals and special occasions?\n",
    "\n",
    "### ğŸ” Market Insights:\n",
    "- Which commodities show the highest price volatility?\n",
    "- How do local vs. imported produce prices compare?\n",
    "- What is the relationship between minimum and maximum prices?\n",
    "\n",
    "### ğŸŒ¾ Economic Analysis:\n",
    "- How do weather patterns affect vegetable prices?\n",
    "- What is the impact of supply chain disruptions on prices?\n",
    "- Can we identify inflationary trends in food prices?\n",
    "\n",
    "### ğŸ¤– Machine Learning Applications:\n",
    "- Price forecasting models for different commodities\n",
    "- Anomaly detection in price patterns\n",
    "- Clustering analysis of similar price behaviors\n",
    "\n",
    "---\n",
    "\n",
    "**Let's explore the data and uncover the stories hidden in Nepal's agricultural market!** ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘¤ Author\n",
    "\n",
    "**Sajjad Ali Shah**  \n",
    "Data Scientist | Machine Learning Engineer  \n",
    "ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/sajjad-ali-shah47/)\n",
    "\n",
    "*Feel free to connect for collaborations, discussions, or questions about this analysis!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e887a",
   "metadata": {},
   "source": [
    "## ğŸ”— Dataset Source\n",
    "\n",
    "ğŸ“¦ **Kaggle**: [Kalimati Tarkari Dataset](https://www.kaggle.com/datasets/nischallal/kalimati-tarkari-dataset)\n",
    "\n",
    "Access the complete dataset on Kaggle for your analysis and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19b7e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ”„ Time Series Analysis Workflow\n",
    "\n",
    "Following a systematic approach for time series analysis:\n",
    "1. ğŸ“¥ **Data Collection** - Load the dataset\n",
    "2. ğŸ“… **Datetime Handling** - Convert and parse date columns\n",
    "3. ğŸ” **Initial Data Inspection** - Understand data structure\n",
    "4. â“ **Missing Values Check** - Identify temporal gaps\n",
    "5. ğŸ“Š **Resampling** (if needed) - Standardize time intervals\n",
    "6. ğŸ“ˆ **Exploratory Data Analysis** - Discover patterns and trends\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1ï¸âƒ£: Data Collection\n",
    "\n",
    "Loading the Kalimati Tarkari dataset for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06eb3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57aafc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk451\\AppData\\Local\\Temp\\ipykernel_14572\\3741286561.py:2: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('Dataset/Kalimati_Tarkari_Dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df=pd.read_csv('Dataset/Kalimati_Tarkari_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4363d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2ï¸âƒ£: Datetime Handling\n",
    "\n",
    "Converting date columns to proper datetime format and extracting temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime format\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ“… DATETIME HANDLING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check original date format\n",
    "print(f\"\\nğŸ” Original Date Column:\")\n",
    "print(f\"Data Type: {df['Date'].dtype}\")\n",
    "print(f\"Sample values:\\n{df['Date'].head()}\")\n",
    "\n",
    "# Convert to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=False)\n",
    "\n",
    "print(f\"\\nâœ… After Conversion:\")\n",
    "print(f\"Data Type: {df['Date'].dtype}\")\n",
    "print(f\"Sample values:\\n{df['Date'].head()}\")\n",
    "\n",
    "# Extract temporal features\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Month_Name'] = df['Date'].dt.month_name()\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Day_of_Week'] = df['Date'].dt.day_name()\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "\n",
    "print(f\"\\nğŸ“Š Date Range:\")\n",
    "print(f\"Start Date: {df['Date'].min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"End Date: {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total Days: {(df['Date'].max() - df['Date'].min()).days:,}\")\n",
    "print(f\"Total Years: {(df['Date'].max() - df['Date'].min()).days / 365.25:.2f}\")\n",
    "\n",
    "print(f\"\\nâœ… New Temporal Features Created:\")\n",
    "print(f\"   â€¢ Year, Month, Month_Name, Day\")\n",
    "print(f\"   â€¢ Day_of_Week, Quarter, Week_of_Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c8a77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3ï¸âƒ£: Initial Data Inspection\n",
    "\n",
    "Understanding the structure and basic characteristics of the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8840cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ“Š DATASET INFORMATION\n",
      "====================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 280862 entries, 0 to 280861\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Commodity  280862 non-null  object\n",
      " 1   Date       280862 non-null  object\n",
      " 2   Unit       280862 non-null  object\n",
      " 3   Minimum    280862 non-null  object\n",
      " 4   Maximum    280862 non-null  object\n",
      " 5   Average    280862 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 12.9+ MB\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ DATASET SHAPE\n",
      "====================================================================================================\n",
      "Rows: 280,862\n",
      "Columns: 6\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ” FIRST 5 ROWS\n",
      "====================================================================================================\n",
      "             Commodity       Date Unit Minimum Maximum Average\n",
      "0   Tomato Big(Nepali)  6/16/2013   Kg      35      40    37.5\n",
      "1  Tomato Small(Local)  6/16/2013   Kg      26      32    29.0\n",
      "2           Potato Red  6/16/2013   Kg      20      21    20.5\n",
      "3         Potato White  6/16/2013   Kg      15      16    15.5\n",
      "4   Onion Dry (Indian)  6/16/2013   Kg      28      30    29.0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ˆ STATISTICAL SUMMARY\n",
      "====================================================================================================\n",
      "          Commodity        Date    Unit  Minimum  Maximum   Average\n",
      "count        280862      280862  280862   280862   280862  280862.0\n",
      "unique          136        3615       6      493      514    1529.0\n",
      "top     Cauli Local  2023-03-19      Kg       50       60      95.0\n",
      "freq           3612         115  212344    10213    11236    8375.0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ” MISSING VALUES\n",
      "====================================================================================================\n",
      "âœ… No missing values found!\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š DUPLICATE ROWS\n",
      "====================================================================================================\n",
      "Total duplicate rows: 0\n",
      "âœ… No duplicate rows found!\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ·ï¸ UNIQUE VALUES PER COLUMN\n",
      "====================================================================================================\n",
      "Commodity: 136 unique values\n",
      "Date: 3,615 unique values\n",
      "Unit: 6 unique values\n",
      "Minimum: 493 unique values\n",
      "Maximum: 514 unique values\n",
      "Average: 1,529 unique values\n"
     ]
    }
   ],
   "source": [
    "# Dataset Overview\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ“Š DATASET INFORMATION\")\n",
    "print(\"=\" * 100)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“ DATASET SHAPE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ” FIRST 5 ROWS\")\n",
    "print(\"=\" * 100)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“ˆ STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ” MISSING VALUES\")\n",
    "print(\"=\" * 100)\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“Š DUPLICATE ROWS\")\n",
    "print(\"=\" * 100)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicates:,}\")\n",
    "if duplicates == 0:\n",
    "    print(\"âœ… No duplicate rows found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ·ï¸ UNIQUE VALUES PER COLUMN\")\n",
    "print(\"=\" * 100)\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique():,} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9bbd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” EXPLORING COLUMN: Commodity\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ Column Data Type: object\n",
      "ğŸ”¢ Total Values: 280,862\n",
      "ğŸ·ï¸ Unique Values: 136\n",
      "â“ Missing Values: 0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ CATEGORICAL DATA - Top 20 Values\n",
      "====================================================================================================\n",
      "Commodity\n",
      "Cauli Local             3612\n",
      "Ginger                  3612\n",
      "Chilli Dry              3609\n",
      "Banana                  3604\n",
      "Coriander Green         3603\n",
      "Bamboo Shoot            3603\n",
      "Potato Red              3602\n",
      "Brd Leaf Mustard        3602\n",
      "French Bean(Local)      3600\n",
      "Cabbage(Local)          3600\n",
      "Carrot(Local)           3596\n",
      "Onion Green             3593\n",
      "Chilli Green            3592\n",
      "Garlic Dry Chinese      3589\n",
      "Raddish White(Local)    3588\n",
      "Brinjal Long            3586\n",
      "Lime                    3585\n",
      "Mushroom(Kanya)         3584\n",
      "Onion Dry (Indian)      3582\n",
      "Tomato Small(Local)     3581\n",
      "Name: count, dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š SAMPLE VALUES\n",
      "====================================================================================================\n",
      "['Tomato Big(Nepali)', 'Tomato Small(Local)', 'Potato Red', 'Potato White', 'Onion Dry (Indian)', 'Carrot(Local)', 'Cabbage(Local)', 'Cauli Local', 'Raddish Red', 'Raddish White(Local)']\n"
     ]
    }
   ],
   "source": [
    "# Explore a specific column in detail\n",
    "# Change the column_name variable to explore different columns\n",
    "\n",
    "column_name = 'Commodity'  # Change this to any column name you want to explore\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"ğŸ” EXPLORING COLUMN: {column_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if column_name not in df.columns:\n",
    "    print(f\"âŒ Error: Column '{column_name}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‹ Column Data Type: {df[column_name].dtype}\")\n",
    "    print(f\"ğŸ”¢ Total Values: {len(df[column_name]):,}\")\n",
    "    print(f\"ğŸ·ï¸ Unique Values: {df[column_name].nunique():,}\")\n",
    "    print(f\"â“ Missing Values: {df[column_name].isnull().sum():,}\")\n",
    "    \n",
    "    if df[column_name].dtype == 'object':\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ CATEGORICAL DATA - Top 20 Values\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].value_counts().head(20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š SAMPLE VALUES\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].head(10).tolist())\n",
    "        \n",
    "    elif np.issubdtype(df[column_name].dtype, np.number):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š NUMERICAL DATA - Statistical Summary\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].describe())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ˆ DISTRIBUTION\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Mean: {df[column_name].mean():.2f}\")\n",
    "        print(f\"Median: {df[column_name].median():.2f}\")\n",
    "        print(f\"Mode: {df[column_name].mode()[0] if not df[column_name].mode().empty else 'N/A'}\")\n",
    "        print(f\"Standard Deviation: {df[column_name].std():.2f}\")\n",
    "        print(f\"Min: {df[column_name].min():.2f}\")\n",
    "        print(f\"Max: {df[column_name].max():.2f}\")\n",
    "        print(f\"Range: {df[column_name].max() - df[column_name].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd236f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4ï¸âƒ£: Missing Values in Time Series\n",
    "\n",
    "Identifying temporal gaps and missing data points in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8720be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Missing Values Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"â“ MISSING VALUES IN TIME SERIES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Check for missing values in columns\n",
    "print(\"\\nğŸ“Š Missing Values by Column:\")\n",
    "print(\"-\" * 100)\n",
    "missing_count = df.isnull().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_count.index,\n",
    "    'Missing Count': missing_count.values,\n",
    "    'Missing %': missing_percent.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].to_string(index=False) if missing_df['Missing Count'].sum() > 0 else \"âœ… No missing values in columns!\")\n",
    "\n",
    "# 2. Check for temporal gaps (missing dates)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“… Temporal Continuity Check:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Sort by date\n",
    "df_sorted = df.sort_values('Date')\n",
    "\n",
    "# Get all unique dates\n",
    "unique_dates = df_sorted['Date'].unique()\n",
    "date_range = pd.date_range(start=df_sorted['Date'].min(), end=df_sorted['Date'].max(), freq='D')\n",
    "\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(unique_dates))\n",
    "\n",
    "print(f\"Expected date range: {len(date_range):,} days\")\n",
    "print(f\"Actual unique dates: {len(unique_dates):,} days\")\n",
    "print(f\"Missing dates: {len(missing_dates):,} days\")\n",
    "\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"\\nâš ï¸ Found {len(missing_dates)} missing dates in the time series\")\n",
    "    print(f\"First 10 missing dates:\")\n",
    "    for date in missing_dates[:10]:\n",
    "        print(f\"  â€¢ {date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No temporal gaps found - continuous daily data!\")\n",
    "\n",
    "# 3. Check data frequency\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“Š Time Series Frequency Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Unique Commodities: {df['Commodity'].nunique()}\")\n",
    "print(f\"Records per Commodity (average): {len(df) / df['Commodity'].nunique():.2f}\")\n",
    "\n",
    "# 4. Check for irregular spacing\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“ˆ Data Distribution Over Time:\")\n",
    "print(\"-\" * 100)\n",
    "records_per_year = df.groupby('Year').size()\n",
    "print(records_per_year)\n",
    "\n",
    "print(\"\\nâœ… Missing values analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b20309",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5ï¸âƒ£: Resampling (If Needed)\n",
    "\n",
    "Checking if resampling is required and standardizing time intervals if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bf8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ“Š RESAMPLING ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check current frequency\n",
    "print(f\"\\nğŸ” Current Data Frequency:\")\n",
    "print(f\"   â€¢ Data appears to be DAILY (one record per commodity per day)\")\n",
    "print(f\"   â€¢ Multiple commodities tracked simultaneously\")\n",
    "\n",
    "# Example: Resample one commodity to different frequencies\n",
    "sample_commodity = df['Commodity'].value_counts().index[0]\n",
    "commodity_data = df[df['Commodity'] == sample_commodity].set_index('Date').sort_index()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Sample Resampling for '{sample_commodity}':\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Daily (current)\n",
    "print(f\"\\n1ï¸âƒ£ Daily Frequency (Current):\")\n",
    "print(f\"   Records: {len(commodity_data)}\")\n",
    "print(f\"   Sample:\\n{commodity_data[['Average']].head()}\")\n",
    "\n",
    "# Weekly resampling\n",
    "weekly = commodity_data[['Minimum', 'Maximum', 'Average']].resample('W').mean()\n",
    "print(f\"\\n2ï¸âƒ£ Weekly Resampling (Mean prices):\")\n",
    "print(f\"   Records: {len(weekly)}\")\n",
    "print(f\"   Sample:\\n{weekly.head()}\")\n",
    "\n",
    "# Monthly resampling\n",
    "monthly = commodity_data[['Minimum', 'Maximum', 'Average']].resample('M').mean()\n",
    "print(f\"\\n3ï¸âƒ£ Monthly Resampling (Mean prices):\")\n",
    "print(f\"   Records: {len(monthly)}\")\n",
    "print(f\"   Sample:\\n{monthly.head()}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Resampling Options:\")\n",
    "print(f\"   â€¢ Daily (D) - Current frequency âœ…\")\n",
    "print(f\"   â€¢ Weekly (W) - For weekly trend analysis\")\n",
    "print(f\"   â€¢ Monthly (M) - For long-term patterns\")\n",
    "print(f\"   â€¢ Quarterly (Q) - For seasonal analysis\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ Decision: Keep DAILY frequency for detailed analysis\")\n",
    "print(f\"   (Can resample later for specific visualizations or modeling)\")\n",
    "\n",
    "print(\"\\nâœ… Resampling analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e8acc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6ï¸âƒ£: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Deep dive into the time series data to discover patterns, trends, and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b33f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” EXPLORING COLUMN: Unit\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ Column Data Type: object\n",
      "ğŸ”¢ Total Values: 280,862\n",
      "ğŸ·ï¸ Unique Values: 6\n",
      "â“ Missing Values: 0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ CATEGORICAL DATA - Top 20 Values\n",
      "====================================================================================================\n",
      "Unit\n",
      "Kg           212344\n",
      "KG            60732\n",
      "1 Pc           4056\n",
      "Doz            3080\n",
      "Per Dozen       524\n",
      "Per Piece       126\n",
      "Name: count, dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š SAMPLE VALUES\n",
      "====================================================================================================\n",
      "['Kg', 'Kg', 'Kg', 'Kg', 'Kg', 'Kg', 'Kg', 'Kg', 'Kg', 'Kg']\n"
     ]
    }
   ],
   "source": [
    "# Explore a specific column in detail\n",
    "# Change the column_name variable to explore different columns\n",
    "\n",
    "column_name = 'Unit'  # Change this to any column name you want to explore\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"ğŸ” EXPLORING COLUMN: {column_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if column_name not in df.columns:\n",
    "    print(f\"âŒ Error: Column '{column_name}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‹ Column Data Type: {df[column_name].dtype}\")\n",
    "    print(f\"ğŸ”¢ Total Values: {len(df[column_name]):,}\")\n",
    "    print(f\"ğŸ·ï¸ Unique Values: {df[column_name].nunique():,}\")\n",
    "    print(f\"â“ Missing Values: {df[column_name].isnull().sum():,}\")\n",
    "    \n",
    "    if df[column_name].dtype == 'object':\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ CATEGORICAL DATA - Top 20 Values\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].value_counts().head(20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š SAMPLE VALUES\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].head(10).tolist())\n",
    "        \n",
    "    elif np.issubdtype(df[column_name].dtype, np.number):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š NUMERICAL DATA - Statistical Summary\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].describe())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ˆ DISTRIBUTION\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Mean: {df[column_name].mean():.2f}\")\n",
    "        print(f\"Median: {df[column_name].median():.2f}\")\n",
    "        print(f\"Mode: {df[column_name].mode()[0] if not df[column_name].mode().empty else 'N/A'}\")\n",
    "        print(f\"Standard Deviation: {df[column_name].std():.2f}\")\n",
    "        print(f\"Min: {df[column_name].min():.2f}\")\n",
    "        print(f\"Max: {df[column_name].max():.2f}\")\n",
    "        print(f\"Range: {df[column_name].max() - df[column_name].min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a05172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” EXPLORING COLUMN: Average\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ Column Data Type: object\n",
      "ğŸ”¢ Total Values: 280,862\n",
      "ğŸ·ï¸ Unique Values: 1,529\n",
      "â“ Missing Values: 0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ CATEGORICAL DATA - Top 20 Values\n",
      "====================================================================================================\n",
      "Average\n",
      "95.0     8375\n",
      "55       7089\n",
      "37.5     6806\n",
      "47.5     6309\n",
      "95       6165\n",
      "32.5     5908\n",
      "55.0     5885\n",
      "75.0     5859\n",
      "65.0     5661\n",
      "42.5     5559\n",
      "105.0    5413\n",
      "57.5     5320\n",
      "75       5235\n",
      "27.5     4499\n",
      "52.5     4222\n",
      "65       4217\n",
      "85.0     3998\n",
      "85       3920\n",
      "45       3913\n",
      "45.0     3607\n",
      "Name: count, dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š SAMPLE VALUES\n",
      "====================================================================================================\n",
      "[37.5, 29.0, 20.5, 15.5, 29.0, 32.5, 8.0, 32.5, 37.5, 27.5]\n"
     ]
    }
   ],
   "source": [
    "# Explore a specific column in detail\n",
    "# Change the column_name variable to explore different columns\n",
    "\n",
    "column_name = 'Average'  # Change this to any column name you want to explore\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"ğŸ” EXPLORING COLUMN: {column_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if column_name not in df.columns:\n",
    "    print(f\"âŒ Error: Column '{column_name}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‹ Column Data Type: {df[column_name].dtype}\")\n",
    "    print(f\"ğŸ”¢ Total Values: {len(df[column_name]):,}\")\n",
    "    print(f\"ğŸ·ï¸ Unique Values: {df[column_name].nunique():,}\")\n",
    "    print(f\"â“ Missing Values: {df[column_name].isnull().sum():,}\")\n",
    "    \n",
    "    if df[column_name].dtype == 'object':\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ CATEGORICAL DATA - Top 20 Values\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].value_counts().head(20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š SAMPLE VALUES\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].head(10).tolist())\n",
    "        \n",
    "    elif np.issubdtype(df[column_name].dtype, np.number):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š NUMERICAL DATA - Statistical Summary\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].describe())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ˆ DISTRIBUTION\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Mean: {df[column_name].mean():.2f}\")\n",
    "        print(f\"Median: {df[column_name].median():.2f}\")\n",
    "        print(f\"Mode: {df[column_name].mode()[0] if not df[column_name].mode().empty else 'N/A'}\")\n",
    "        print(f\"Standard Deviation: {df[column_name].std():.2f}\")\n",
    "        print(f\"Min: {df[column_name].min():.2f}\")\n",
    "        print(f\"Max: {df[column_name].max():.2f}\")\n",
    "        print(f\"Range: {df[column_name].max() - df[column_name].min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5f39a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” EXPLORING COLUMN: Minimum\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ Column Data Type: object\n",
      "ğŸ”¢ Total Values: 280,862\n",
      "ğŸ·ï¸ Unique Values: 493\n",
      "â“ Missing Values: 0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ CATEGORICAL DATA - Top 20 Values\n",
      "====================================================================================================\n",
      "Minimum\n",
      "50          10213\n",
      "40           9529\n",
      "90           9125\n",
      "60           8836\n",
      "30           8558\n",
      "50           7802\n",
      "35           7177\n",
      "70           7177\n",
      "45           6581\n",
      "90           6280\n",
      "100          5800\n",
      "40           5760\n",
      "70           5532\n",
      "55           5468\n",
      "80           4924\n",
      "25           4877\n",
      "60           4753\n",
      "30           4564\n",
      "Rs 50.00     4364\n",
      "80           4309\n",
      "Name: count, dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š SAMPLE VALUES\n",
      "====================================================================================================\n",
      "[35, 26, 20, 15, 28, 30, 6, 30, 35, 25]\n"
     ]
    }
   ],
   "source": [
    "# Explore a specific column in detail\n",
    "# Change the column_name variable to explore different columns\n",
    "\n",
    "column_name = 'Minimum'  # Change this to any column name you want to explore\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"ğŸ” EXPLORING COLUMN: {column_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if column_name not in df.columns:\n",
    "    print(f\"âŒ Error: Column '{column_name}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‹ Column Data Type: {df[column_name].dtype}\")\n",
    "    print(f\"ğŸ”¢ Total Values: {len(df[column_name]):,}\")\n",
    "    print(f\"ğŸ·ï¸ Unique Values: {df[column_name].nunique():,}\")\n",
    "    print(f\"â“ Missing Values: {df[column_name].isnull().sum():,}\")\n",
    "    \n",
    "    if df[column_name].dtype == 'object':\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ CATEGORICAL DATA - Top 20 Values\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].value_counts().head(20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š SAMPLE VALUES\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].head(10).tolist())\n",
    "        \n",
    "    elif np.issubdtype(df[column_name].dtype, np.number):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š NUMERICAL DATA - Statistical Summary\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].describe())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ˆ DISTRIBUTION\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Mean: {df[column_name].mean():.2f}\")\n",
    "        print(f\"Median: {df[column_name].median():.2f}\")\n",
    "        print(f\"Mode: {df[column_name].mode()[0] if not df[column_name].mode().empty else 'N/A'}\")\n",
    "        print(f\"Standard Deviation: {df[column_name].std():.2f}\")\n",
    "        print(f\"Min: {df[column_name].min():.2f}\")\n",
    "        print(f\"Max: {df[column_name].max():.2f}\")\n",
    "        print(f\"Range: {df[column_name].max() - df[column_name].min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d9ec969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” EXPLORING COLUMN: Maximum\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“‹ Column Data Type: object\n",
      "ğŸ”¢ Total Values: 280,862\n",
      "ğŸ·ï¸ Unique Values: 514\n",
      "â“ Missing Values: 0\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ CATEGORICAL DATA - Top 20 Values\n",
      "====================================================================================================\n",
      "Maximum\n",
      "60          11236\n",
      "50           9954\n",
      "40           9334\n",
      "100          9227\n",
      "60           8685\n",
      "80           7647\n",
      "100          7470\n",
      "50           7015\n",
      "70           6818\n",
      "80           6784\n",
      "35           6228\n",
      "30           6069\n",
      "45           5831\n",
      "40           5632\n",
      "110          5535\n",
      "70           5216\n",
      "120          4760\n",
      "55           4416\n",
      "90           4357\n",
      "Rs 60.00     4215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š SAMPLE VALUES\n",
      "====================================================================================================\n",
      "[40, 32, 21, 16, 30, 35, 10, 35, 40, 30]\n"
     ]
    }
   ],
   "source": [
    "# Explore a specific column in detail\n",
    "# Change the column_name variable to explore different columns\n",
    "\n",
    "column_name = 'Maximum'  # Change this to any column name you want to explore\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"ğŸ” EXPLORING COLUMN: {column_name}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "if column_name not in df.columns:\n",
    "    print(f\"âŒ Error: Column '{column_name}' not found in dataset!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“‹ Column Data Type: {df[column_name].dtype}\")\n",
    "    print(f\"ğŸ”¢ Total Values: {len(df[column_name]):,}\")\n",
    "    print(f\"ğŸ·ï¸ Unique Values: {df[column_name].nunique():,}\")\n",
    "    print(f\"â“ Missing Values: {df[column_name].isnull().sum():,}\")\n",
    "    \n",
    "    if df[column_name].dtype == 'object':\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ CATEGORICAL DATA - Top 20 Values\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].value_counts().head(20))\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š SAMPLE VALUES\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].head(10).tolist())\n",
    "        \n",
    "    elif np.issubdtype(df[column_name].dtype, np.number):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“Š NUMERICAL DATA - Statistical Summary\")\n",
    "        print(\"=\" * 100)\n",
    "        print(df[column_name].describe())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"ğŸ“ˆ DISTRIBUTION\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Mean: {df[column_name].mean():.2f}\")\n",
    "        print(f\"Median: {df[column_name].median():.2f}\")\n",
    "        print(f\"Mode: {df[column_name].mode()[0] if not df[column_name].mode().empty else 'N/A'}\")\n",
    "        print(f\"Standard Deviation: {df[column_name].std():.2f}\")\n",
    "        print(f\"Min: {df[column_name].min():.2f}\")\n",
    "        print(f\"Max: {df[column_name].max():.2f}\")\n",
    "        print(f\"Range: {df[column_name].max() - df[column_name].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556cef2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Data Quality Check\n",
    "\n",
    "Performing comprehensive data quality checks to ensure data integrity and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1307a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ” DATA QUALITY CHECK\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Missing Values Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "âœ… No missing values detected!\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ”„ Duplicate Rows Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total duplicate rows: 0 (0.00%)\n",
      "âœ… No duplicate rows found!\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ Data Types Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Commodity    object\n",
      "Date         object\n",
      "Unit         object\n",
      "Minimum      object\n",
      "Maximum      object\n",
      "Average      object\n",
      "dtype: object\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ’° Price Columns Validation:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ˆ Outliers Detection (IQR Method):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ·ï¸ Categorical Data Consistency:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Commodity: 136 unique values\n",
      "Date: 3,615 unique values\n",
      "Unit: 6 unique values\n",
      "Minimum: 493 unique values\n",
      "Maximum: 514 unique values\n",
      "Average: 1,529 unique values\n",
      "\n",
      "====================================================================================================\n",
      "âœ… Data Quality Check Complete!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Check - Comprehensive Analysis\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ” DATA QUALITY CHECK\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Check for missing values percentage\n",
    "print(\"\\nğŸ“Š Missing Values Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "missing_count = df.isnull().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_count.index,\n",
    "    'Missing Count': missing_count.values,\n",
    "    'Missing %': missing_percent.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"âœ… No missing values detected!\")\n",
    "\n",
    "# 2. Check for duplicate rows\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ”„ Duplicate Rows Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count:,} ({(duplicate_count/len(df)*100):.2f}%)\")\n",
    "if duplicate_count > 0:\n",
    "    print(\"âš ï¸ Action Required: Consider removing duplicates\")\n",
    "else:\n",
    "    print(\"âœ… No duplicate rows found!\")\n",
    "\n",
    "# 3. Data types check\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“ Data Types Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(df.dtypes)\n",
    "\n",
    "# 4. Check for negative values in price columns (if they exist)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ’° Price Columns Validation:\")\n",
    "print(\"-\" * 100)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    negative_count = (df[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"âš ï¸ {col}: Found {negative_count} negative values\")\n",
    "    else:\n",
    "        print(f\"âœ… {col}: No negative values\")\n",
    "\n",
    "# 5. Check for outliers using IQR method\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ“ˆ Outliers Detection (IQR Method):\")\n",
    "print(\"-\" * 100)\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_percent = (outlier_count / len(df)) * 100\n",
    "    print(f\"{col}: {outlier_count:,} outliers ({outlier_percent:.2f}%)\")\n",
    "\n",
    "# 6. Check for inconsistencies in categorical columns\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ğŸ·ï¸ Categorical Data Consistency:\")\n",
    "print(\"-\" * 100)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count:,} unique values\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"âœ… Data Quality Check Complete!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24083c05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's dive deep into the data to uncover patterns, trends, and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90745e0c",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Date Analysis\n",
    "Understanding the temporal coverage of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f572ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"2022-04-19\" doesn't match format \"%m/%d/%Y\", at position 3090. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert Date column to datetime format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“… DATE ANALYSIS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1068\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1066\u001b[39m             result = arg.tz_localize(\u001b[33m\"\u001b[39m\u001b[33mutc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m     cache_array = \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array.empty:\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:249\u001b[39m, in \u001b[36m_maybe_cache\u001b[39m\u001b[34m(arg, format, cache, convert_listlike)\u001b[39m\n\u001b[32m    247\u001b[39m unique_dates = unique(arg)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) < \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     cache_dates = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    438\u001b[39m     arg,\n\u001b[32m    439\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    444\u001b[39m )\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:469\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    459\u001b[39m     arg,\n\u001b[32m    460\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    465\u001b[39m ) -> Index:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    471\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:583\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: time data \"2022-04-19\" doesn't match format \"%m/%d/%Y\", at position 3090. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Convert Date column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ“… DATE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nğŸ“† Date Range:\")\n",
    "print(f\"   Start Date: {df['Date'].min()}\")\n",
    "print(f\"   End Date: {df['Date'].max()}\")\n",
    "print(f\"   Total Days: {(df['Date'].max() - df['Date'].min()).days:,}\")\n",
    "print(f\"   Total Years: {(df['Date'].max() - df['Date'].min()).days / 365.25:.2f}\")\n",
    "\n",
    "# Extract date components\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Month_Name'] = df['Date'].dt.month_name()\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Day_of_Week'] = df['Date'].dt.day_name()\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "print(f\"\\nğŸ“Š Records Per Year:\")\n",
    "print(df['Year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nğŸ“Š Records Per Month:\")\n",
    "print(df['Month_Name'].value_counts().reindex([\n",
    "    'January', 'February', 'March', 'April', 'May', 'June',\n",
    "    'July', 'August', 'September', 'October', 'November', 'December'\n",
    "]))\n",
    "\n",
    "print(\"\\nâœ… Date processing complete!\")\n",
    "print(f\"New columns added: Year, Month, Month_Name, Day, Day_of_Week, Quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6bf869",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ Commodity Analysis\n",
    "Analyzing the different types of commodities in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2900334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ¥¬ COMMODITY ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Total Unique Commodities: 136\n",
      "\n",
      "ğŸ“ˆ Top 20 Most Frequently Recorded Commodities:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " 1. Cauli Local                    -  3,612 records\n",
      " 2. Ginger                         -  3,612 records\n",
      " 3. Chilli Dry                     -  3,609 records\n",
      " 4. Banana                         -  3,604 records\n",
      " 5. Coriander Green                -  3,603 records\n",
      " 6. Bamboo Shoot                   -  3,603 records\n",
      " 7. Potato Red                     -  3,602 records\n",
      " 8. Brd Leaf Mustard               -  3,602 records\n",
      " 9. French Bean(Local)             -  3,600 records\n",
      "10. Cabbage(Local)                 -  3,600 records\n",
      "11. Carrot(Local)                  -  3,596 records\n",
      "12. Onion Green                    -  3,593 records\n",
      "13. Chilli Green                   -  3,592 records\n",
      "14. Garlic Dry Chinese             -  3,589 records\n",
      "15. Raddish White(Local)           -  3,588 records\n",
      "16. Brinjal Long                   -  3,586 records\n",
      "17. Lime                           -  3,585 records\n",
      "18. Mushroom(Kanya)                -  3,584 records\n",
      "19. Onion Dry (Indian)             -  3,582 records\n",
      "20. Tomato Small(Local)            -  3,581 records\n",
      "\n",
      "ğŸ“Š Unit Types Distribution:\n",
      "Unit\n",
      "Kg           212344\n",
      "KG            60732\n",
      "1 Pc           4056\n",
      "Doz            3080\n",
      "Per Dozen       524\n",
      "Per Piece       126\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ’¡ Sample Commodities by Unit Type:\n",
      "\n",
      "Kg:\n",
      "  â€¢ Tomato Big(Nepali)\n",
      "  â€¢ Tomato Small(Local)\n",
      "  â€¢ Potato Red\n",
      "  â€¢ Potato White\n",
      "  â€¢ Onion Dry (Indian)\n",
      "\n",
      "Doz:\n",
      "  â€¢ Banana\n",
      "\n",
      "1 Pc:\n",
      "  â€¢ Pineapple\n",
      "  â€¢ Sugarcane\n",
      "  â€¢ Maize\n",
      "\n",
      "KG:\n",
      "  â€¢ Potato Red(Indian)\n",
      "  â€¢ Mushroom(Button)\n",
      "  â€¢ Apple(Fuji)\n",
      "  â€¢ Cucumber(Hybrid)\n",
      "  â€¢ Chilli Green(Bullet)\n",
      "\n",
      "Per Dozen:\n",
      "  â€¢ Banana\n",
      "\n",
      "Per Piece:\n",
      "  â€¢ Sugarcane\n"
     ]
    }
   ],
   "source": [
    "# Commodity Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ¥¬ COMMODITY ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nğŸ“Š Total Unique Commodities: {df['Commodity'].nunique()}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Top 20 Most Frequently Recorded Commodities:\")\n",
    "print(\"-\" * 100)\n",
    "top_commodities = df['Commodity'].value_counts().head(20)\n",
    "for idx, (commodity, count) in enumerate(top_commodities.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - {count:6,} records\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Unit Types Distribution:\")\n",
    "print(df['Unit'].value_counts())\n",
    "\n",
    "print(f\"\\nğŸ’¡ Sample Commodities by Unit Type:\")\n",
    "for unit in df['Unit'].unique():\n",
    "    commodities = df[df['Unit'] == unit]['Commodity'].unique()[:5]\n",
    "    print(f\"\\n{unit}:\")\n",
    "    for commodity in commodities:\n",
    "        print(f\"  â€¢ {commodity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26052601",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Price Analysis\n",
    "Analyzing price distributions and trends across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ead865f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ğŸ’° PRICE ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š Overall Price Statistics:\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š Overall Price Statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m100\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Minimum Price: â‚¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMinimum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Maximum Price: â‚¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33mMaximum\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Price: â‚¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33mAverage\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\series.py:6570\u001b[39m, in \u001b[36mSeries.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m   6562\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m1\u001b[39m))\n\u001b[32m   6563\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m   6564\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6568\u001b[39m     **kwargs,\n\u001b[32m   6569\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m6570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\generic.py:12485\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  12479\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12480\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12483\u001b[39m     **kwargs,\n\u001b[32m  12484\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12486\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\series.py:6478\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   6473\u001b[39m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[32m   6474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   6475\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwith non-numeric dtypes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6477\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m6478\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    716\u001b[39m     dtype_count = dtype\n\u001b[32m    718\u001b[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m the_sum = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m the_sum = _ensure_numeric(the_sum)\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_install\\envs\\llm\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     48\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# Price Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"ğŸ’° PRICE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Overall price statistics\n",
    "print(f\"\\nğŸ“Š Overall Price Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Average Minimum Price: â‚¨ {df['Minimum'].mean():.2f}\")\n",
    "print(f\"Average Maximum Price: â‚¨ {df['Maximum'].mean():.2f}\")\n",
    "print(f\"Average Price: â‚¨ {df['Average'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Price Range Statistics:\")\n",
    "df['Price_Range'] = df['Maximum'] - df['Minimum']\n",
    "print(f\"Average Price Range: â‚¨ {df['Price_Range'].mean():.2f}\")\n",
    "print(f\"Median Price Range: â‚¨ {df['Price_Range'].median():.2f}\")\n",
    "print(f\"Max Price Range: â‚¨ {df['Price_Range'].max():.2f}\")\n",
    "\n",
    "# Calculate price volatility (coefficient of variation)\n",
    "df['Price_Volatility'] = (df['Price_Range'] / df['Average']) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 10 Commodities by Average Price:\")\n",
    "print(\"-\" * 100)\n",
    "top_priced = df.groupby('Commodity')['Average'].mean().sort_values(ascending=False).head(10)\n",
    "for idx, (commodity, price) in enumerate(top_priced.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - â‚¨ {price:.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Top 10 Most Volatile Commodities (by price range):\")\n",
    "print(\"-\" * 100)\n",
    "most_volatile = df.groupby('Commodity')['Price_Volatility'].mean().sort_values(ascending=False).head(10)\n",
    "for idx, (commodity, volatility) in enumerate(most_volatile.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - {volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250602c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
