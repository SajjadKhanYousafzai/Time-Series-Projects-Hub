{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4ed4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13315cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    'AEP': './Data/AEP_hourly.csv',\n",
    "    'COMED': './Data/COMED_hourly.csv',\n",
    "    'DAYTON': './Data/DAYTON_hourly.csv',\n",
    "    'DEOK': './Data/DEOK_hourly.csv',\n",
    "    'DOM': './Data/DOM_hourly.csv',\n",
    "    'DUQ': './Data/DUQ_hourly.csv',\n",
    "    'EKPC': './Data/EKPC_hourly.csv',\n",
    "    'FE': './Data/FE_hourly.csv',\n",
    "    'NI': './Data/NI_hourly.csv',\n",
    "    'PJME': './Data/PJME_hourly.csv',\n",
    "    'PJMW': './Data/PJMW_hourly.csv',\n",
    "    'pjm_est': './Data/pjm_hourly_est.csv',\n",
    "    'PJM_Load': './Data/PJM_Load_hourly.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94705a0",
   "metadata": {},
   "source": [
    "# ðŸ“Š AEP Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEP Dataset Analysis\n",
    "name = 'AEP'\n",
    "filepath = data_files['AEP']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "df_aep = pd.read_csv(filepath)\n",
    "\n",
    "# 1. Basic Info\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_aep.shape[0]:,} rows Ã— {df_aep.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_aep.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_aep.dtypes)\n",
    "\n",
    "# 2. Missing Values\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_aep.isnull().sum()\n",
    "missing_pct = (df_aep.isnull().sum() / len(df_aep)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "# 3. Descriptive Statistics\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_aep.describe())\n",
    "\n",
    "# 4. Date Range\n",
    "if 'Datetime' in df_aep.columns:\n",
    "    df_aep['Datetime'] = pd.to_datetime(df_aep['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_aep['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_aep['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_aep['Datetime'].max() - df_aep['Datetime'].min()).days} days\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609c3cb",
   "metadata": {},
   "source": [
    "# ðŸ“Š COMED Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMED Dataset Analysis\n",
    "name = 'COMED'\n",
    "filepath = data_files['COMED']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load data\n",
    "df_comed = pd.read_csv(filepath)\n",
    "\n",
    "# 1. Basic Info\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_comed.shape[0]:,} rows Ã— {df_comed.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_comed.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_comed.dtypes)\n",
    "\n",
    "# 2. Missing Values\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_comed.isnull().sum()\n",
    "missing_pct = (df_comed.isnull().sum() / len(df_comed)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "# 3. Descriptive Statistics\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_comed.describe())\n",
    "\n",
    "# 4. Date Range\n",
    "if 'Datetime' in df_comed.columns:\n",
    "    df_comed['Datetime'] = pd.to_datetime(df_comed['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_comed['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_comed['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_comed['Datetime'].max() - df_comed['Datetime'].min()).days} days\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462abeb",
   "metadata": {},
   "source": [
    "# ðŸ“Š DAYTON Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c47248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAYTON Dataset Analysis\n",
    "name = 'DAYTON'\n",
    "filepath = data_files['DAYTON']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_dayton = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_dayton.shape[0]:,} rows Ã— {df_dayton.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_dayton.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_dayton.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_dayton.isnull().sum()\n",
    "missing_pct = (df_dayton.isnull().sum() / len(df_dayton)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_dayton.describe())\n",
    "\n",
    "if 'Datetime' in df_dayton.columns:\n",
    "    df_dayton['Datetime'] = pd.to_datetime(df_dayton['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_dayton['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_dayton['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_dayton['Datetime'].max() - df_dayton['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d997a",
   "metadata": {},
   "source": [
    "# ðŸ“Š DEOK Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEOK Dataset Analysis\n",
    "name = 'DEOK'\n",
    "filepath = data_files['DEOK']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_deok = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_deok.shape[0]:,} rows Ã— {df_deok.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_deok.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_deok.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_deok.isnull().sum()\n",
    "missing_pct = (df_deok.isnull().sum() / len(df_deok)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_deok.describe())\n",
    "\n",
    "if 'Datetime' in df_deok.columns:\n",
    "    df_deok['Datetime'] = pd.to_datetime(df_deok['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_deok['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_deok['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_deok['Datetime'].max() - df_deok['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f1f05",
   "metadata": {},
   "source": [
    "# ðŸ“Š DOM Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977929d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOM Dataset Analysis\n",
    "name = 'DOM'\n",
    "filepath = data_files['DOM']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_dom = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_dom.shape[0]:,} rows Ã— {df_dom.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_dom.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_dom.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_dom.isnull().sum()\n",
    "missing_pct = (df_dom.isnull().sum() / len(df_dom)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_dom.describe())\n",
    "\n",
    "if 'Datetime' in df_dom.columns:\n",
    "    df_dom['Datetime'] = pd.to_datetime(df_dom['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_dom['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_dom['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_dom['Datetime'].max() - df_dom['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b4608",
   "metadata": {},
   "source": [
    "# ðŸ“Š DUQ Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUQ Dataset Analysis\n",
    "name = 'DUQ'\n",
    "filepath = data_files['DUQ']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_duq = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_duq.shape[0]:,} rows Ã— {df_duq.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_duq.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_duq.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_duq.isnull().sum()\n",
    "missing_pct = (df_duq.isnull().sum() / len(df_duq)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_duq.describe())\n",
    "\n",
    "if 'Datetime' in df_duq.columns:\n",
    "    df_duq['Datetime'] = pd.to_datetime(df_duq['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_duq['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_duq['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_duq['Datetime'].max() - df_duq['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dac99",
   "metadata": {},
   "source": [
    "# ðŸ“Š EKPC Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EKPC Dataset Analysis\n",
    "name = 'EKPC'\n",
    "filepath = data_files['EKPC']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_ekpc = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_ekpc.shape[0]:,} rows Ã— {df_ekpc.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_ekpc.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_ekpc.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_ekpc.isnull().sum()\n",
    "missing_pct = (df_ekpc.isnull().sum() / len(df_ekpc)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_ekpc.describe())\n",
    "\n",
    "if 'Datetime' in df_ekpc.columns:\n",
    "    df_ekpc['Datetime'] = pd.to_datetime(df_ekpc['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_ekpc['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_ekpc['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_ekpc['Datetime'].max() - df_ekpc['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748035d1",
   "metadata": {},
   "source": [
    "# ðŸ“Š FE Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6094ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FE Dataset Analysis\n",
    "name = 'FE'\n",
    "filepath = data_files['FE']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_fe = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_fe.shape[0]:,} rows Ã— {df_fe.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_fe.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_fe.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_fe.isnull().sum()\n",
    "missing_pct = (df_fe.isnull().sum() / len(df_fe)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_fe.describe())\n",
    "\n",
    "if 'Datetime' in df_fe.columns:\n",
    "    df_fe['Datetime'] = pd.to_datetime(df_fe['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_fe['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_fe['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_fe['Datetime'].max() - df_fe['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721b5c1",
   "metadata": {},
   "source": [
    "# ðŸ“Š NI Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NI Dataset Analysis\n",
    "name = 'NI'\n",
    "filepath = data_files['NI']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_ni = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_ni.shape[0]:,} rows Ã— {df_ni.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_ni.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_ni.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_ni.isnull().sum()\n",
    "missing_pct = (df_ni.isnull().sum() / len(df_ni)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_ni.describe())\n",
    "\n",
    "if 'Datetime' in df_ni.columns:\n",
    "    df_ni['Datetime'] = pd.to_datetime(df_ni['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_ni['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_ni['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_ni['Datetime'].max() - df_ni['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24125761",
   "metadata": {},
   "source": [
    "# ðŸ“Š PJME Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PJME Dataset Analysis\n",
    "name = 'PJME'\n",
    "filepath = data_files['PJME']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pjme = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_pjme.shape[0]:,} rows Ã— {df_pjme.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_pjme.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_pjme.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_pjme.isnull().sum()\n",
    "missing_pct = (df_pjme.isnull().sum() / len(df_pjme)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_pjme.describe())\n",
    "\n",
    "if 'Datetime' in df_pjme.columns:\n",
    "    df_pjme['Datetime'] = pd.to_datetime(df_pjme['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_pjme['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_pjme['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_pjme['Datetime'].max() - df_pjme['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173f82c",
   "metadata": {},
   "source": [
    "# ðŸ“Š PJMW Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b62134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PJMW Dataset Analysis\n",
    "name = 'PJMW'\n",
    "filepath = data_files['PJMW']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pjmw = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_pjmw.shape[0]:,} rows Ã— {df_pjmw.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_pjmw.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_pjmw.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_pjmw.isnull().sum()\n",
    "missing_pct = (df_pjmw.isnull().sum() / len(df_pjmw)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_pjmw.describe())\n",
    "\n",
    "if 'Datetime' in df_pjmw.columns:\n",
    "    df_pjmw['Datetime'] = pd.to_datetime(df_pjmw['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_pjmw['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_pjmw['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_pjmw['Datetime'].max() - df_pjmw['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031d033",
   "metadata": {},
   "source": [
    "# ðŸ“Š pjm_est Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pjm_est Dataset Analysis\n",
    "name = 'pjm_est'\n",
    "filepath = data_files['pjm_est']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pjm_est = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_pjm_est.shape[0]:,} rows Ã— {df_pjm_est.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_pjm_est.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_pjm_est.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_pjm_est.isnull().sum()\n",
    "missing_pct = (df_pjm_est.isnull().sum() / len(df_pjm_est)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_pjm_est.describe())\n",
    "\n",
    "if 'Datetime' in df_pjm_est.columns:\n",
    "    df_pjm_est['Datetime'] = pd.to_datetime(df_pjm_est['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_pjm_est['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_pjm_est['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_pjm_est['Datetime'].max() - df_pjm_est['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9cad5",
   "metadata": {},
   "source": [
    "# ðŸ“Š PJM_Load Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PJM_Load Dataset Analysis\n",
    "name = 'PJM_Load'\n",
    "filepath = data_files['PJM_Load']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š DATASET: {name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pjm_load = pd.read_csv(filepath)\n",
    "\n",
    "print(f\"\\nðŸ” BASIC INFO for {name}:\")\n",
    "print(f\"   Shape: {df_pjm_load.shape[0]:,} rows Ã— {df_pjm_load.shape[1]} columns\")\n",
    "print(f\"   Columns: {list(df_pjm_load.columns)}\")\n",
    "print(f\"\\n   Data Types:\")\n",
    "print(df_pjm_load.dtypes)\n",
    "\n",
    "print(f\"\\nâŒ MISSING VALUES for {name}:\")\n",
    "missing = df_pjm_load.isnull().sum()\n",
    "missing_pct = (df_pjm_load.isnull().sum() / len(df_pjm_load)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0] if missing.sum() > 0 else \"   âœ… No missing values!\")\n",
    "print(f\"   Total Missing: {missing.sum():,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DESCRIPTIVE STATISTICS for {name}:\")\n",
    "print(df_pjm_load.describe())\n",
    "\n",
    "if 'Datetime' in df_pjm_load.columns:\n",
    "    df_pjm_load['Datetime'] = pd.to_datetime(df_pjm_load['Datetime'])\n",
    "    print(f\"\\nðŸ“… DATE RANGE for {name}:\")\n",
    "    print(f\"   Start: {df_pjm_load['Datetime'].min()}\")\n",
    "    print(f\"   End: {df_pjm_load['Datetime'].max()}\")\n",
    "    print(f\"   Duration: {(df_pjm_load['Datetime'].max() - df_pjm_load['Datetime'].min()).days} days\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889c619",
   "metadata": {},
   "source": [
    "# ðŸ“‹ Comprehensive Summary Table\n",
    "Compare all datasets side-by-side with key metrics and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840911ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "ðŸ“Š COMPREHENSIVE SUMMARY OF ALL DATASETS\n",
      "========================================================================================================================\n",
      "\n",
      " Dataset   Rows  Columns  Missing Values Missing %          Start Date   End Date  Duration (days)     Mean      Min      Max Std Dev\n",
      "     AEP 121273        2               0     0.00% 2004-10-01 01:00:00 2018-08-03             5053 15499.51  9581.00 25695.00 2591.40\n",
      "   COMED  66497        2               0     0.00% 2011-01-01 01:00:00 2018-08-03             2770 11420.15  7237.00 23753.00 2304.14\n",
      "  DAYTON 121275        2               0     0.00% 2004-10-01 01:00:00 2018-08-03             5053  2037.85   982.00  3746.00  393.40\n",
      "    DEOK  57739        2               0     0.00% 2012-01-01 01:00:00 2018-08-03             2405  3105.10   907.00  5445.00  599.86\n",
      "     DOM 116189        2               0     0.00% 2005-05-01 01:00:00 2018-08-03             4841 10949.20  1253.00 21651.00 2413.95\n",
      "     DUQ 119068        2               0     0.00% 2005-01-01 01:00:00 2018-08-03             4961  1658.82  1014.00  3054.00  301.74\n",
      "    EKPC  45334        2               0     0.00% 2013-06-01 01:00:00 2018-08-03             1888  1464.22   514.00  3490.00  378.87\n",
      "      FE  62874        2               0     0.00% 2011-06-01 01:00:00 2018-08-03             2619  7792.16     0.00 14032.00 1331.27\n",
      "      NI  58450        2               0     0.00% 2004-05-01 01:00:00 2011-01-01             2435 11701.68  7003.00 23631.00 2371.50\n",
      "    PJME 145366        2               0     0.00% 2002-01-01 01:00:00 2018-08-03             6057 32080.22 14544.00 62009.00 6464.01\n",
      "    PJMW 143206        2               0     0.00% 2002-04-01 01:00:00 2018-08-03             5967  5602.38   487.00  9594.00  979.14\n",
      " pjm_est 178262       13         1048977    45.27% 1998-04-01 01:00:00 2018-08-03             7428 15499.51  9581.00 25695.00 2591.40\n",
      "PJM_Load  32896        2               0     0.00% 1998-04-01 01:00:00 2002-01-01             1370 29766.43 17461.00 54030.00 5849.77\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive summary table for all datasets\n",
    "summary_data = []\n",
    "\n",
    "for name, filepath in data_files.items():\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert datetime if exists\n",
    "    if 'Datetime' in df.columns:\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        date_start = df['Datetime'].min()\n",
    "        date_end = df['Datetime'].max()\n",
    "        duration_days = (date_end - date_start).days\n",
    "    else:\n",
    "        date_start = date_end = duration_days = 'N/A'\n",
    "    \n",
    "    # Get energy column name (usually the second column)\n",
    "    energy_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dataset': name,\n",
    "        'Rows': df.shape[0],\n",
    "        'Columns': df.shape[1],\n",
    "        'Missing Values': df.isnull().sum().sum(),\n",
    "        'Missing %': f\"{(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\",\n",
    "        'Start Date': date_start,\n",
    "        'End Date': date_end,\n",
    "        'Duration (days)': duration_days,\n",
    "        'Mean': f\"{df[energy_col].mean():.2f}\" if pd.api.types.is_numeric_dtype(df[energy_col]) else 'N/A',\n",
    "        'Min': f\"{df[energy_col].min():.2f}\" if pd.api.types.is_numeric_dtype(df[energy_col]) else 'N/A',\n",
    "        'Max': f\"{df[energy_col].max():.2f}\" if pd.api.types.is_numeric_dtype(df[energy_col]) else 'N/A',\n",
    "        'Std Dev': f\"{df[energy_col].std():.2f}\" if pd.api.types.is_numeric_dtype(df[energy_col]) else 'N/A'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ðŸ“Š COMPREHENSIVE SUMMARY OF ALL DATASETS\")\n",
    "print(\"=\"*120 + \"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
