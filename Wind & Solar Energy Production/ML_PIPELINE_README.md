# ğŸ¯ Enhanced ML Pipeline: 70-20-10 Split + Advanced Models

## ğŸ“Š Overview

This enhanced machine learning pipeline implements a **production-ready approach** for energy production forecasting with:

- âœ… **Proper 70-20-10 train-validation-test split** (temporal ordering preserved)
- âœ… **Time series windowing** for sequence-based models
- âœ… **Three ML models**: Random Forest, XGBoost, and LSTM
- âœ… **Comprehensive evaluation** across all data splits
- âœ… **Interactive dashboard** for model comparison

---

## ğŸš€ What's New?

### 1. Proper Data Splitting

```
Total Dataset: 51,864 samples
â”œâ”€â”€ Training Set:   70% (36,305 samples)
â”œâ”€â”€ Validation Set: 20% (10,373 samples)
â””â”€â”€ Test Set:       10% (5,186 samples)
```

**Why this matters:**
- **Training**: Learn patterns from historical data
- **Validation**: Tune hyperparameters without touching test data
- **Test**: Final unbiased performance evaluation
- **Temporal ordering**: No data leakage (past â†’ future)

### 2. Time Series Windowing

Creates sliding windows for sequence-based models (LSTM):

```python
Window Size: 24 hours (look-back period)
Forecast Horizon: 1 hour ahead

Input Shape:  (samples, 24, 3)  # 24 timesteps, 3 features
Output Shape: (samples, 1)      # Predict next hour
```

### 3. Feature Engineering

**For Tree-Based Models (RF, XGBoost):**
- Cyclical encoding (hour, day of year)
- Lag features (t-1, t-24, t-168)
- Rolling statistics (24-hour mean/std)
- Categorical encoding (source, season, month, day)

**Total Features:** 17 engineered features

### 4. Three ML Models

| Model | Type | Strengths | Training Time |
|-------|------|-----------|---------------|
| **Random Forest** | Ensemble Trees | Robust, interpretable, no scaling needed | ~30 seconds |
| **XGBoost** | Gradient Boosting | High accuracy, handles missing values | ~45 seconds |
| **LSTM** | Deep Learning | Captures sequential patterns, temporal dependencies | ~3-5 minutes |

---

## ğŸ“ˆ Model Performance

### Test Set Results

| Model | MAE (MWh) | RMSE (MWh) | MAPE (%) | RÂ² Score |
|-------|-----------|------------|----------|----------|
| Random Forest | TBD | TBD | TBD | TBD |
| XGBoost | TBD | TBD | TBD | TBD |
| LSTM | TBD | TBD | TBD | TBD |

*Run the notebook to see actual results!*

### Metrics Explained

- **MAE** (Mean Absolute Error): Average prediction error in MWh
- **RMSE** (Root Mean Square Error): Penalizes large errors more
- **MAPE** (Mean Absolute Percentage Error): Percentage-based error metric
- **RÂ²** (R-squared): Proportion of variance explained (0-1, higher is better)

---

## ğŸ¨ Dashboard Features

### Main Dashboard (`/`)
- Real-time predictions with Random Forest model
- Live forecast visualization
- Interactive prediction form
- Model performance metrics

### Models Comparison (`/models-comparison`)
- Side-by-side comparison of all 3 models
- Performance metrics visualization
- Test set predictions overlay
- Best model identification per metric
- Feature importance analysis

---

## ğŸ—‚ï¸ Files Generated

### Models Directory (`models/`)
```
models/
â”œâ”€â”€ random_forest_model.pkl      # Trained Random Forest
â”œâ”€â”€ xgboost_model.pkl            # Trained XGBoost
â”œâ”€â”€ lstm_model.h5                # Trained LSTM (Keras format)
â”œâ”€â”€ lstm_scaler.pkl              # StandardScaler for LSTM
â””â”€â”€ model_metadata.json          # All metrics and config
```

### Dashboard Data (`Dashboard/data/`)
```
Dashboard/data/
â”œâ”€â”€ predictions.csv              # Test predictions from all models
â”œâ”€â”€ model_comparison.json        # Performance comparison
â””â”€â”€ feature_importance.json      # Feature importance rankings
```

---

## ğŸ› ï¸ Installation & Setup

### 1. Install Required Packages

```bash
pip install numpy pandas matplotlib seaborn scikit-learn xgboost tensorflow joblib
```

### 2. Run the Notebook

Open `wind_solar_energy.ipynb` and run the new cells starting from:

```
ğŸ¯ ENHANCED ML PIPELINE: 70-20-10 SPLIT + ADVANCED MODELS
```

### 3. Start the Dashboard

```bash
cd Dashboard/frontend
npm install
npm run dev
```

Navigate to:
- Main Dashboard: http://localhost:3000
- Models Comparison: http://localhost:3000/models-comparison

---

## ğŸ“Š How to Use

### Step-by-Step Guide

1. **Run Data Split Cell**
   - Creates train/validation/test sets
   - Preserves temporal ordering

2. **Create Time Series Windows**
   - Generates 24-hour sequences for LSTM
   - Maintains data structure

3. **Feature Engineering**
   - Builds 17 features for tree models
   - Encodes categorical variables

4. **Train Models**
   - Random Forest: Fast, robust baseline
   - XGBoost: Best for tabular data
   - LSTM: Captures temporal patterns

5. **Compare Performance**
   - Visualize predictions
   - Analyze metrics
   - Identify best model

6. **Save & Deploy**
   - Export models and predictions
   - Integrate with dashboard
   - Make live predictions

---

## ğŸ¯ Key Insights

### Why This Approach?

1. **Validation Set Prevents Overfitting**
   - Tune hyperparameters without biasing test results
   - Essential for model selection

2. **Windowing Captures Temporal Dependencies**
   - LSTM learns from past 24 hours
   - Recognizes daily patterns

3. **Multiple Models = Ensemble Potential**
   - Different models learn different patterns
   - Can be combined for better predictions

4. **Production-Ready**
   - Saved models can be loaded instantly
   - Dashboard provides user interface
   - Scalable architecture

---

## ğŸ“š Best Practices Implemented

âœ… **No Data Leakage**: Test data never seen during training
âœ… **Temporal Ordering**: Past data predicts future, not vice versa
âœ… **Comprehensive Evaluation**: All metrics across all splits
âœ… **Feature Engineering**: Domain knowledge + automation
âœ… **Model Persistence**: Save/load trained models
âœ… **Visualization**: Clear charts for interpretation
âœ… **Documentation**: Well-commented code

---

## ğŸš€ Next Steps

### Further Improvements

1. **Hyperparameter Tuning**
   ```python
   from sklearn.model_selection import GridSearchCV
   # Optimize RF and XGBoost parameters
   ```

2. **Ensemble Methods**
   ```python
   # Combine predictions from all 3 models
   ensemble_pred = 0.4*rf + 0.4*xgb + 0.2*lstm
   ```

3. **Cross-Validation**
   ```python
   from sklearn.model_selection import TimeSeriesSplit
   # 5-fold time series CV
   ```

4. **External Features**
   - Weather data (temperature, wind speed)
   - Calendar features (holidays)
   - Economic indicators

5. **Advanced Models**
   - Prophet (Facebook's forecasting)
   - Transformer-based models
   - ARIMA for comparison

---

## ğŸ¤ Contributing

Feel free to:
- Experiment with different window sizes
- Add more models (Prophet, Transformer)
- Improve feature engineering
- Enhance dashboard visualizations

---

## ğŸ“ Citation

If you use this pipeline, please reference:

```
Wind & Solar Energy Production ML Pipeline
70-20-10 Split with Random Forest, XGBoost, and LSTM
Dataset: France Renewable Energy (2020-2025)
```

---

## ğŸŠ Summary

This enhanced pipeline transforms a basic time series analysis into a **production-ready ML system** with:

- Proper train/val/test splits
- Multiple state-of-the-art models
- Comprehensive evaluation framework
- Interactive dashboard
- Saved models ready for deployment

**Perfect for:**
- Academic projects
- Portfolio demonstrations
- Industry applications
- Energy grid management
- Research papers

---

**Built with â¤ï¸ for robust, reproducible, and production-ready ML**
