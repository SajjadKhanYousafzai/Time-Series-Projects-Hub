{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c534cd3",
   "metadata": {},
   "source": [
    "# ðŸ“Š Store Sales Time Series Forecasting\n",
    "\n",
    "## ðŸŽ¯ Competition Overview\n",
    "\n",
    "In this competition, we will predict sales for thousands of product families sold at **Favorita stores** located in Ecuador. The training data includes dates, store and product information, promotional status, and sales numbers. Additional supplementary files are provided to enhance model building.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‘ Table of Contents\n",
    "\n",
    "1. **Setup & Data Loading** â€” Import libraries, load all datasets\n",
    "2. **Data Exploration & Quality** â€” Shape, types, missing values, duplicates\n",
    "3. **Deep Exploratory Data Analysis (EDA)**\n",
    "   - Overall sales trends (daily, monthly, yearly)\n",
    "   - Sales by product family, store, city, state\n",
    "   - Oil price analysis & correlation with sales\n",
    "   - Holiday & event impact\n",
    "   - Promotion impact analysis\n",
    "   - Wage payment day effect (15th & last of month)\n",
    "   - 2016 Ecuador earthquake impact\n",
    "   - Time series decomposition\n",
    "   - Day-of-week & monthly patterns\n",
    "   - Transaction analysis\n",
    "4. **Key Observations Summary** â€” Non-technical summary of findings\n",
    "5. **Feature Engineering** â€” Building a modeling-ready dataset\n",
    "6. **Modeling & Evaluation** â€” Multiple models compared\n",
    "7. **Best Model & Final Predictions**\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/sajjad-ali-shah47/)\n",
    "[![Kaggle](https://img.shields.io/badge/Kaggle-Dataset-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Sajjad Ali Shah | **Project Type:** Time Series Forecasting | **Tools:** Python, Pandas, Scikit-learn, LightGBM\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4766087",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Setup & Data Loading\n",
    "\n",
    "Let's start by importing all necessary libraries and loading our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901503b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“¦ Import Libraries\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Time series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 13\n",
    "\n",
    "# Color palette\n",
    "COLORS = sns.color_palette(\"husl\", 12)\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84460ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“‚ Load All Datasets\n",
    "# ============================================================\n",
    "DATA_PATH = 'Dataset/'\n",
    "\n",
    "train = pd.read_csv(f'{DATA_PATH}train.csv', parse_dates=['date'])\n",
    "test = pd.read_csv(f'{DATA_PATH}test.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(f'{DATA_PATH}stores.csv')\n",
    "oil = pd.read_csv(f'{DATA_PATH}oil.csv', parse_dates=['date'])\n",
    "holidays = pd.read_csv(f'{DATA_PATH}holidays_events.csv', parse_dates=['date'])\n",
    "transactions = pd.read_csv(f'{DATA_PATH}transactions.csv', parse_dates=['date'])\n",
    "submission = pd.read_csv(f'{DATA_PATH}sample_submission.csv')\n",
    "\n",
    "print(\"âœ… All datasets loaded successfully!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“Š Dataset Shapes:\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, df in [('Train', train), ('Test', test), ('Stores', stores),\n",
    "                  ('Oil', oil), ('Holidays/Events', holidays),\n",
    "                  ('Transactions', transactions), ('Sample Submission', submission)]:\n",
    "    print(f\"   â€¢ {name:20s}: {str(df.shape):>15s}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nðŸ“… Training period: {train['date'].min().date()} to {train['date'].max().date()}\")\n",
    "print(f\"ðŸ“… Test period:     {test['date'].min().date()} to {test['date'].max().date()}\")\n",
    "print(f\"ðŸª Number of stores: {train['store_nbr'].nunique()}\")\n",
    "print(f\"ðŸ“¦ Product families:  {train['family'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d7537",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Data Exploration & Quality\n",
    "\n",
    "Let's examine each dataset in detail â€” understanding data types, missing values, and basic statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efd2c2",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data overview\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "display(train.head(10))\n",
    "print(f\"\\nShape: {train.shape}\")\n",
    "print(f\"\\nData Types:\\n{train.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{train.isnull().sum()}\")\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "display(train.describe())\n",
    "print(f\"\\nðŸ“¦ Product Families ({train['family'].nunique()}):\")\n",
    "print(train['family'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c04bbb",
   "metadata": {},
   "source": [
    "### ðŸª Stores Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores overview\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸª STORES DATA\")\n",
    "print(\"=\" * 70)\n",
    "display(stores)\n",
    "print(f\"\\nCities: {stores['city'].nunique()} | States: {stores['state'].nunique()}\")\n",
    "print(f\"Store Types: {stores['type'].unique()}\")\n",
    "print(f\"Clusters: {sorted(stores['cluster'].unique())}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "stores['type'].value_counts().plot(kind='bar', ax=axes[0], color=COLORS[:5], edgecolor='black')\n",
    "axes[0].set_title('ðŸª Stores by Type', fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "stores['state'].value_counts().plot(kind='barh', ax=axes[1], color=COLORS[2], edgecolor='black')\n",
    "axes[1].set_title('ðŸ—ºï¸ Stores by State', fontweight='bold')\n",
    "\n",
    "stores['cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[2], color=COLORS[4], edgecolor='black')\n",
    "axes[2].set_title('ðŸ“Š Stores by Cluster', fontweight='bold')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99da66c",
   "metadata": {},
   "source": [
    "### ðŸ›¢ï¸ Oil Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oil data overview\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ›¢ï¸ OIL PRICE DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Shape: {oil.shape}\")\n",
    "print(f\"Date range: {oil['date'].min().date()} to {oil['date'].max().date()}\")\n",
    "print(f\"\\nMissing values: {oil.isnull().sum().to_dict()}\")\n",
    "print(f\"Missing oil prices: {oil['dcoilwtico'].isnull().sum()} ({oil['dcoilwtico'].isnull().mean()*100:.1f}%)\")\n",
    "\n",
    "# Fill missing oil prices with interpolation\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].interpolate(method='linear')\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].bfill()\n",
    "print(f\"\\nâœ… Oil prices interpolated. Remaining NaN: {oil['dcoilwtico'].isnull().sum()}\")\n",
    "\n",
    "display(oil.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a9ef3",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ Holidays & Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays overview\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸŽ‰ HOLIDAYS & EVENTS DATA\")\n",
    "print(\"=\" * 70)\n",
    "display(holidays.head(10))\n",
    "print(f\"\\nShape: {holidays.shape}\")\n",
    "print(f\"\\nHoliday Types:\\n{holidays['type'].value_counts()}\")\n",
    "print(f\"\\nLocale Types:\\n{holidays['locale'].value_counts()}\")\n",
    "print(f\"\\nTransferred: {holidays['transferred'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495d051",
   "metadata": {},
   "source": [
    "### ðŸ§¾ Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions overview\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ§¾ TRANSACTIONS DATA\")\n",
    "print(\"=\" * 70)\n",
    "display(transactions.head(10))\n",
    "print(f\"\\nShape: {transactions.shape}\")\n",
    "print(f\"Date range: {transactions['date'].min().date()} to {transactions['date'].max().date()}\")\n",
    "display(transactions.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32117c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Deep Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now let's dive deep into the data with professional visualizations. Each chart includes annotations and observations to help **non-technical readers** understand the patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb4dd9",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ 3.1 Overall Sales Trends\n",
    "\n",
    "> **What are we looking at?** Total daily sales across ALL stores over time. This helps us see the big picture â€” growth trends, seasonality, and major disruptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily total sales\n",
    "daily_sales = train.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# Daily\n",
    "axes[0].plot(daily_sales['date'], daily_sales['sales'], linewidth=0.5, alpha=0.7, color=COLORS[0])\n",
    "axes[0].set_title('ðŸ“ˆ Daily Total Sales (All Stores)', fontweight='bold', fontsize=16)\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "axes[0].axvline(pd.Timestamp('2016-04-16'), color='red', linestyle='--', alpha=0.8, label='ðŸŒ Earthquake (Apr 16, 2016)')\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Monthly\n",
    "monthly_sales = train.groupby(train['date'].dt.to_period('M'))['sales'].sum().reset_index()\n",
    "monthly_sales['date'] = monthly_sales['date'].dt.to_timestamp()\n",
    "axes[1].plot(monthly_sales['date'], monthly_sales['sales'], linewidth=2, color=COLORS[1], marker='o', markersize=3)\n",
    "axes[1].set_title('ðŸ“… Monthly Total Sales', fontweight='bold', fontsize=16)\n",
    "axes[1].set_ylabel('Total Sales')\n",
    "axes[1].axvline(pd.Timestamp('2016-04-01'), color='red', linestyle='--', alpha=0.8, label='ðŸŒ Earthquake Month')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "# Yearly\n",
    "yearly_sales = train.groupby(train['date'].dt.year)['sales'].sum().reset_index()\n",
    "yearly_sales.columns = ['year', 'sales']\n",
    "axes[2].bar(yearly_sales['year'].astype(str), yearly_sales['sales'], color=COLORS[2], edgecolor='black', alpha=0.8)\n",
    "axes[2].set_title('ðŸ“Š Yearly Total Sales', fontweight='bold', fontsize=16)\n",
    "axes[2].set_ylabel('Total Sales')\n",
    "for i, v in enumerate(yearly_sales['sales']):\n",
    "    axes[2].text(i, v + v*0.01, f'{v/1e6:.1f}M', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803e5e9",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** Sales show a clear **upward trend** over the years, with strong **seasonal spikes** (likely around holidays like Christmas). The 2016 earthquake caused a visible disruption in the daily sales pattern.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883308b",
   "metadata": {},
   "source": [
    "### ðŸ“¦ 3.2 Sales by Product Family\n",
    "\n",
    "> **What are we looking at?** Which product categories sell the most? This helps the business decide where to focus inventory and promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and Bottom product families\n",
    "family_sales = train.groupby('family')['sales'].sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Top 10\n",
    "top10 = family_sales.head(10)\n",
    "bars1 = axes[0].barh(top10['family'], top10['sales'], color=sns.color_palette('YlOrRd_r', 10), edgecolor='black')\n",
    "axes[0].set_title('ðŸ† Top 10 Product Families by Sales', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Total Sales')\n",
    "axes[0].invert_yaxis()\n",
    "for bar, val in zip(bars1, top10['sales']):\n",
    "    axes[0].text(val + val*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val/1e6:.1f}M', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Bottom 10\n",
    "bot10 = family_sales.tail(10)\n",
    "bars2 = axes[1].barh(bot10['family'], bot10['sales'], color=sns.color_palette('Blues_r', 10), edgecolor='black')\n",
    "axes[1].set_title('ðŸ“‰ Bottom 10 Product Families by Sales', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlabel('Total Sales')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadc4e5",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** **GROCERY I** and **BEVERAGES** dominate sales massively, accounting for a significant share. Some categories like **BOOKS** and **BABY CARE** have very low sales volumes â€” these may be niche products carried in select stores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a18e01",
   "metadata": {},
   "source": [
    "### ðŸª 3.3 Sales by Store, City & State\n",
    "\n",
    "> **What are we looking at?** Which stores and locations generate the most revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef36a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge store info with sales\n",
    "train_stores = train.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Sales by store type\n",
    "type_sales = train_stores.groupby('type')['sales'].mean().sort_values(ascending=False)\n",
    "type_sales.plot(kind='bar', ax=axes[0,0], color=COLORS[:5], edgecolor='black', alpha=0.85)\n",
    "axes[0,0].set_title('ðŸª Average Daily Sales by Store Type', fontweight='bold')\n",
    "axes[0,0].set_ylabel('Average Sales')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Sales by state (top 10)\n",
    "state_sales = train_stores.groupby('state')['sales'].sum().sort_values(ascending=False).head(10)\n",
    "state_sales.plot(kind='barh', ax=axes[0,1], color=COLORS[3], edgecolor='black', alpha=0.85)\n",
    "axes[0,1].set_title('ðŸ—ºï¸ Top 10 States by Total Sales', fontweight='bold')\n",
    "axes[0,1].invert_yaxis()\n",
    "\n",
    "# Sales by city (top 10)\n",
    "city_sales = train_stores.groupby('city')['sales'].sum().sort_values(ascending=False).head(10)\n",
    "city_sales.plot(kind='barh', ax=axes[1,0], color=COLORS[5], edgecolor='black', alpha=0.85)\n",
    "axes[1,0].set_title('ðŸ™ï¸ Top 10 Cities by Total Sales', fontweight='bold')\n",
    "axes[1,0].invert_yaxis()\n",
    "\n",
    "# Sales by cluster\n",
    "cluster_sales = train_stores.groupby('cluster')['sales'].mean().sort_values(ascending=False)\n",
    "cluster_sales.plot(kind='bar', ax=axes[1,1], color=COLORS[7], edgecolor='black', alpha=0.85)\n",
    "axes[1,1].set_title('ðŸ“Š Average Daily Sales by Cluster', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Average Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf059b7a",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** Stores of **Type D** tend to have the highest average daily sales. **Pichincha** (the state containing the capital Quito) leads state-level sales. Quito itself is the top city. Store performance varies significantly by cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8d7f6",
   "metadata": {},
   "source": [
    "### ðŸ›¢ï¸ 3.4 Oil Price Analysis\n",
    "\n",
    "> **Why does oil matter?** Ecuador is an oil-dependent country. When oil prices drop, the economy suffers, and supermarket sales may be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge oil with daily sales\n",
    "daily_sales_oil = daily_sales.merge(oil, on='date', how='left')\n",
    "daily_sales_oil['dcoilwtico'] = daily_sales_oil['dcoilwtico'].interpolate().bfill()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Oil price over time\n",
    "axes[0].plot(daily_sales_oil['date'], daily_sales_oil['dcoilwtico'], color='darkgreen', linewidth=1.5)\n",
    "axes[0].set_title('ðŸ›¢ï¸ Daily Oil Price (WTI Crude)', fontweight='bold', fontsize=16)\n",
    "axes[0].set_ylabel('Price (USD/barrel)')\n",
    "axes[0].fill_between(daily_sales_oil['date'], daily_sales_oil['dcoilwtico'], alpha=0.2, color='green')\n",
    "\n",
    "# Scatter: Oil vs Sales\n",
    "axes[1].scatter(daily_sales_oil['dcoilwtico'], daily_sales_oil['sales'],\n",
    "               alpha=0.15, s=8, color=COLORS[0])\n",
    "axes[1].set_title('ðŸ”— Oil Price vs Total Daily Sales', fontweight='bold', fontsize=16)\n",
    "axes[1].set_xlabel('Oil Price (USD/barrel)')\n",
    "axes[1].set_ylabel('Total Daily Sales')\n",
    "\n",
    "# Correlation\n",
    "corr = daily_sales_oil[['dcoilwtico', 'sales']].corr().iloc[0,1]\n",
    "axes[1].text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=axes[1].transAxes,\n",
    "            fontsize=14, fontweight='bold', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec202f42",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** Oil prices show a dramatic decline from ~$100 in 2014 to below $30 in early 2016, followed by a recovery. The correlation between oil prices and sales appears weak, suggesting that while oil impacts the economy broadly, supermarket sales depend more on local factors like promotions, holidays, and seasons.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a0ddc",
   "metadata": {},
   "source": [
    "### ðŸŽ‰ 3.5 Holiday & Event Impact on Sales\n",
    "\n",
    "> **Do holidays boost or reduce sales?** Some holidays close stores (zero sales), while events like Christmas drive massive spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create holiday flag for training data\n",
    "national_holidays = holidays[\n",
    "    (holidays['locale'] == 'National') &\n",
    "    (holidays['transferred'] == False) &\n",
    "    (holidays['type'].isin(['Holiday', 'Additional', 'Transfer']))\n",
    "]['date'].unique()\n",
    "\n",
    "daily_sales['is_holiday'] = daily_sales['date'].isin(national_holidays)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Average sales: holiday vs non-holiday\n",
    "holiday_avg = daily_sales.groupby('is_holiday')['sales'].mean()\n",
    "bars = axes[0].bar(['Regular Day', 'National Holiday'], holiday_avg.values,\n",
    "                   color=[COLORS[0], COLORS[3]], edgecolor='black', alpha=0.85, width=0.5)\n",
    "axes[0].set_title('ðŸŽ‰ Average Daily Sales: Holiday vs Regular', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Average Total Sales')\n",
    "for bar, val in zip(bars, holiday_avg.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + val*0.01,\n",
    "                f'{val:,.0f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Sales by holiday type\n",
    "holiday_dates = holidays[holidays['type'].isin(['Holiday', 'Additional', 'Transfer', 'Bridge', 'Event'])].copy()\n",
    "daily_with_htype = daily_sales.merge(holiday_dates[['date', 'type']].drop_duplicates('date'),\n",
    "                                      on='date', how='left')\n",
    "daily_with_htype['type'] = daily_with_htype['type'].fillna('Regular')\n",
    "htype_avg = daily_with_htype.groupby('type')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "htype_avg.plot(kind='bar', ax=axes[1], color=sns.color_palette('Set2', len(htype_avg)),\n",
    "              edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('ðŸ“Š Average Sales by Day Type', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Average Total Sales')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962cda20",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** National holidays actually show **lower average sales** compared to regular days, likely because many stores close or operate with reduced hours. **Events** and **Transfer** days (when a holiday celebration is moved) tend to have higher sales â€” people prepare for celebrations!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2cba5",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ 3.6 Promotion Impact on Sales\n",
    "\n",
    "> **Do promotions work?** We compare average sales for items on promotion vs not on promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37249fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promotion impact by family\n",
    "train['has_promo'] = train['onpromotion'] > 0\n",
    "promo_impact = train.groupby(['family', 'has_promo'])['sales'].mean().unstack()\n",
    "promo_impact.columns = ['No Promo', 'With Promo']\n",
    "promo_impact['Lift'] = ((promo_impact['With Promo'] - promo_impact['No Promo']) /\n",
    "                         promo_impact['No Promo'] * 100)\n",
    "promo_impact = promo_impact.sort_values('Lift', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Overall promo effect\n",
    "overall = train.groupby('has_promo')['sales'].mean()\n",
    "bars = axes[0].bar(['No Promotion', 'On Promotion'], overall.values,\n",
    "                   color=[COLORS[0], COLORS[4]], edgecolor='black', alpha=0.85, width=0.5)\n",
    "axes[0].set_title('ðŸŽ¯ Average Sales: Promo vs No Promo', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Average Sales per Record')\n",
    "for bar, val in zip(bars, overall.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + val*0.02,\n",
    "                f'{val:.1f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Top 15 families by promo lift\n",
    "top_lift = promo_impact.head(15)\n",
    "top_lift['Lift'].plot(kind='barh', ax=axes[1], color=COLORS[4], edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('ðŸ“Š Top 15 Families by Promotion Sales Lift (%)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlabel('Sales Lift (%)')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffffd1",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** Promotions significantly boost sales! Items on promotion sell substantially more on average. Some product families see **massive lifts** from promotions, making them ideal candidates for promotional campaigns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fa334",
   "metadata": {},
   "source": [
    "### ðŸ’° 3.7 Wage Payment Day Effect\n",
    "\n",
    "> **When do people get paid?** Public sector wages in Ecuador are paid on the **15th** and the **last day of the month**. We expect sales spikes on these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ff35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payday analysis\n",
    "daily_sales['day'] = daily_sales['date'].dt.day\n",
    "daily_sales['is_month_end'] = daily_sales['date'].dt.is_month_end\n",
    "daily_sales['is_15th'] = daily_sales['day'] == 15\n",
    "daily_sales['is_payday'] = daily_sales['is_15th'] | daily_sales['is_month_end']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Payday vs non-payday\n",
    "payday_avg = daily_sales.groupby('is_payday')['sales'].mean()\n",
    "bars = axes[0].bar(['Regular Day', 'Pay Day (15th/EOM)'], payday_avg.values,\n",
    "                   color=[COLORS[0], COLORS[6]], edgecolor='black', alpha=0.85, width=0.5)\n",
    "axes[0].set_title('ðŸ’° Average Sales: Pay Day vs Regular Day', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Average Total Sales')\n",
    "for bar, val in zip(bars, payday_avg.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + val*0.01,\n",
    "                f'{val:,.0f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Sales by day of month\n",
    "day_avg = daily_sales.groupby('day')['sales'].mean()\n",
    "colors_day = ['red' if d in [15, 28, 29, 30, 31] else COLORS[0] for d in day_avg.index]\n",
    "day_avg.plot(kind='bar', ax=axes[1], color=colors_day, edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('ðŸ“… Average Sales by Day of Month', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Average Total Sales')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb21fe",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** Pay days show **higher average sales** than regular days. The pattern is visible in the day-of-month chart where the **15th and end-of-month days** (colored red) tend to spike. People shop more right after getting paid!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181abbb2",
   "metadata": {},
   "source": [
    "### ðŸŒ 3.8 Impact of the 2016 Ecuador Earthquake\n",
    "\n",
    "> **On April 16, 2016,** a magnitude 7.8 earthquake struck Ecuador. Relief efforts led to mass donations and affected supermarket sales for weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f92be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthquake analysis\n",
    "eq_date = pd.Timestamp('2016-04-16')\n",
    "eq_window = daily_sales[\n",
    "    (daily_sales['date'] >= eq_date - timedelta(days=60)) &\n",
    "    (daily_sales['date'] <= eq_date + timedelta(days=60))\n",
    "].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(eq_window['date'], eq_window['sales'], linewidth=2, color=COLORS[0], marker='o', markersize=3)\n",
    "ax.axvline(eq_date, color='red', linestyle='--', linewidth=2, label='ðŸŒ Earthquake (Apr 16)')\n",
    "ax.axvspan(eq_date, eq_date + timedelta(days=14), alpha=0.15, color='red', label='2 weeks after')\n",
    "ax.set_title('ðŸŒ Sales Around the 2016 Ecuador Earthquake (Â±60 days)', fontweight='bold', fontsize=16)\n",
    "ax.set_ylabel('Total Daily Sales')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Before vs After comparison\n",
    "before = daily_sales[(daily_sales['date'] >= eq_date - timedelta(days=30)) &\n",
    "                     (daily_sales['date'] < eq_date)]['sales'].mean()\n",
    "during = daily_sales[(daily_sales['date'] >= eq_date) &\n",
    "                     (daily_sales['date'] < eq_date + timedelta(days=14))]['sales'].mean()\n",
    "after = daily_sales[(daily_sales['date'] >= eq_date + timedelta(days=14)) &\n",
    "                    (daily_sales['date'] <= eq_date + timedelta(days=60))]['sales'].mean()\n",
    "\n",
    "print(f\"ðŸ“Š Average daily sales comparison:\")\n",
    "print(f\"   Before earthquake (30 days): {before:>12,.0f}\")\n",
    "print(f\"   During/relief (14 days):     {during:>12,.0f}\")\n",
    "print(f\"   After (14-60 days):          {after:>12,.0f}\")\n",
    "print(f\"   Change during relief:        {((during-before)/before*100):>+10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806a281",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** The earthquake had a dramatic effect on sales. In the immediate aftermath, sales patterns changed significantly as relief efforts and emergency buying altered consumer behavior. Sales eventually recovered as the situation stabilized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d81e48",
   "metadata": {},
   "source": [
    "### ðŸ”¬ 3.9 Time Series Decomposition\n",
    "\n",
    "> **Breaking down the signal:** We decompose the sales time series into **Trend** (long-term direction), **Seasonality** (repeating patterns), and **Residual** (random noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use weekly aggregation for cleaner decomposition\n",
    "weekly_sales = daily_sales.set_index('date')['sales'].resample('W').sum()\n",
    "\n",
    "decomp = seasonal_decompose(weekly_sales, model='multiplicative', period=52)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 14))\n",
    "components = [('Observed', decomp.observed), ('Trend', decomp.trend),\n",
    "              ('Seasonal', decomp.seasonal), ('Residual', decomp.resid)]\n",
    "comp_colors = [COLORS[0], COLORS[1], COLORS[3], COLORS[5]]\n",
    "\n",
    "for ax, (name, data), color in zip(axes, components, comp_colors):\n",
    "    ax.plot(data, color=color, linewidth=1.5)\n",
    "    ax.set_title(f'ðŸ“Š {name}', fontweight='bold', fontsize=14)\n",
    "    ax.set_ylabel(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7330305",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** The decomposition reveals: (1) A clear **upward trend** in sales over the years, (2) Strong **yearly seasonality** with peaks around December (holiday season), and (3) The residual shows the earthquake disruption as an anomaly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032f204",
   "metadata": {},
   "source": [
    "### ðŸ“… 3.10 Day-of-Week & Monthly Patterns\n",
    "\n",
    "> **When do people shop most?** These cyclical patterns are crucial for accurate forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df729731",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Day of week\n",
    "daily_sales['day_of_week'] = daily_sales['date'].dt.day_name()\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_avg = daily_sales.groupby('day_of_week')['sales'].mean().reindex(dow_order)\n",
    "dow_colors = [COLORS[i] for i in range(7)]\n",
    "dow_avg.plot(kind='bar', ax=axes[0], color=dow_colors, edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('ðŸ“… Average Sales by Day of Week', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('Average Total Sales')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Monthly pattern\n",
    "daily_sales['month'] = daily_sales['date'].dt.month_name()\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "month_avg = daily_sales.groupby('month')['sales'].mean().reindex(month_order)\n",
    "month_avg.plot(kind='bar', ax=axes[1], color=sns.color_palette('coolwarm', 12), edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('ðŸ“… Average Sales by Month', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Average Total Sales')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73e51a",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** **Sundays** are the busiest shopping day, while sales tend to be lower mid-week. **December** has the highest monthly average (holiday shopping), and there's also a visible spike in **April** (possibly Easter-related).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c477d",
   "metadata": {},
   "source": [
    "### ðŸ§¾ 3.11 Transaction Analysis\n",
    "\n",
    "> **How do transactions correlate with sales?** More transactions should mean more sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01861c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions with daily store sales\n",
    "store_daily = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "store_trans = store_daily.merge(transactions, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter: transactions vs sales\n",
    "axes[0].scatter(store_trans['transactions'], store_trans['sales'],\n",
    "               alpha=0.05, s=5, color=COLORS[0])\n",
    "corr_ts = store_trans[['transactions', 'sales']].corr().iloc[0,1]\n",
    "axes[0].set_title(f'ðŸ”— Transactions vs Sales (corr={corr_ts:.3f})', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Daily Transactions')\n",
    "axes[0].set_ylabel('Daily Sales')\n",
    "\n",
    "# Average transactions by day of week\n",
    "trans_dow = transactions.copy()\n",
    "trans_dow['dow'] = trans_dow['date'].dt.day_name()\n",
    "dow_trans = trans_dow.groupby('dow')['transactions'].mean().reindex(dow_order)\n",
    "dow_trans.plot(kind='bar', ax=axes[1], color=dow_colors, edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('ðŸ§¾ Average Transactions by Day of Week', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Average Transactions')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88743f1",
   "metadata": {},
   "source": [
    "**ðŸ” Observation:** There is a **strong positive correlation** between transactions and sales â€” more customers visiting means more revenue. The day-of-week pattern for transactions mirrors the sales pattern, confirming that **Sunday** is the busiest day.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1baf03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Key Observations Summary\n",
    "\n",
    "Here is a **plain-language summary** of everything we discovered â€” written so anyone can understand it, even without a technical background.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š What Drives Sales?\n",
    "\n",
    "| Factor | Impact | Details |\n",
    "|--------|--------|---------|\n",
    "| **Product Family** | ðŸ”´ Very High | GROCERY I and BEVERAGES account for the lion's share of sales |\n",
    "| **Promotions** | ðŸ”´ Very High | Items on promotion sell significantly more â€” promotions work! |\n",
    "| **Day of Week** | ðŸŸ¡ Medium | Sunday is the busiest; mid-week is slower |\n",
    "| **Pay Days** | ðŸŸ¡ Medium | Sales spike on the 15th and last day of month (payday) |\n",
    "| **Holidays** | ðŸŸ¡ Medium | Holidays can reduce sales (store closures) but events increase them |\n",
    "| **Season/Month** | ðŸŸ¡ Medium | December (Christmas) is the peak month |\n",
    "| **Store Type/Location** | ðŸŸ¡ Medium | Type D stores and Quito perform best |\n",
    "| **Transactions** | ðŸ”´ Very High | Strong correlation â€” more visitors = more sales |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ›¢ï¸ Oil Price & the Economy\n",
    "- Oil prices show major fluctuations but have a **weak direct correlation** with daily sales\n",
    "- The impact is more **indirect** â€” through the broader economy, employment, and consumer confidence\n",
    "- Oil price crashed from $100 to below $30 during 2014-2016, coinciding with slower sales growth\n",
    "\n",
    "### ðŸŒ The 2016 Earthquake\n",
    "- The April 16, 2016 earthquake **disrupted normal sales patterns** for several weeks\n",
    "- Immediate aftermath showed unusual buying patterns (relief supplies)\n",
    "- Sales eventually normalized but the event is a clear outlier in the data\n",
    "\n",
    "### ðŸ“ˆ Long-Term Trend\n",
    "- Sales show a **consistent upward trend** from 2013 to 2017\n",
    "- Strong **yearly seasonality** with December peaks\n",
    "- The business is growing overall, which is a positive sign\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0481e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Feature Engineering\n",
    "\n",
    "Now we build a **modeling-ready dataset** by creating meaningful features from our raw data. Good features are the key to accurate forecasting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ”§ Feature Engineering\n",
    "# ============================================================\n",
    "\n",
    "# Start with train data\n",
    "df = train.copy()\n",
    "\n",
    "# ---------- Date Features ----------\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "df['is_payday'] = ((df['day'] == 15) | df['date'].dt.is_month_end).astype(int)\n",
    "\n",
    "# Earthquake flag\n",
    "eq_start = pd.Timestamp('2016-04-16')\n",
    "eq_end = pd.Timestamp('2016-05-15')\n",
    "df['is_earthquake_period'] = ((df['date'] >= eq_start) & (df['date'] <= eq_end)).astype(int)\n",
    "\n",
    "print(\"âœ… Date features created\")\n",
    "print(f\"   Features: year, month, day, day_of_week, day_of_year, week_of_year\")\n",
    "print(f\"   Flags: is_weekend, is_month_start, is_month_end, is_payday, is_earthquake_period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Merge Store Info ----------\n",
    "df = df.merge(stores, on='store_nbr', how='left')\n",
    "print(f\"âœ… Store info merged: city, state, type, cluster\")\n",
    "\n",
    "# ---------- Merge Oil Prices ----------\n",
    "df = df.merge(oil, on='date', how='left')\n",
    "df['dcoilwtico'] = df['dcoilwtico'].interpolate(method='linear').bfill().ffill()\n",
    "print(f\"âœ… Oil prices merged and interpolated\")\n",
    "\n",
    "# ---------- Holiday Features ----------\n",
    "# Create a simplified holiday indicator\n",
    "nat_holidays = holidays[\n",
    "    (holidays['locale'] == 'National') &\n",
    "    (holidays['type'].isin(['Holiday', 'Additional', 'Transfer']))\n",
    "]\n",
    "df['is_national_holiday'] = df['date'].isin(nat_holidays['date'].unique()).astype(int)\n",
    "\n",
    "events = holidays[holidays['type'] == 'Event']\n",
    "df['is_event'] = df['date'].isin(events['date'].unique()).astype(int)\n",
    "\n",
    "print(f\"âœ… Holiday features created: is_national_holiday, is_event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6103eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Encode Categorical Variables ----------\n",
    "le_family = LabelEncoder()\n",
    "le_city = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "\n",
    "df['family_encoded'] = le_family.fit_transform(df['family'])\n",
    "df['city_encoded'] = le_city.fit_transform(df['city'])\n",
    "df['state_encoded'] = le_state.fit_transform(df['state'])\n",
    "df['type_encoded'] = le_type.fit_transform(df['type'])\n",
    "\n",
    "print(f\"âœ… Categorical variables encoded\")\n",
    "print(f\"   Families: {df['family_encoded'].nunique()}, Cities: {df['city_encoded'].nunique()}\")\n",
    "print(f\"   States: {df['state_encoded'].nunique()}, Types: {df['type_encoded'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d35614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Lag & Rolling Features (aggregated by store+family) ----------\n",
    "# Sort for proper lag computation\n",
    "df = df.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Create lags and rolling features per store-family combination\n",
    "for lag in [7, 14, 28]:\n",
    "    df[f'sales_lag_{lag}'] = df.groupby(['store_nbr', 'family_encoded'])['sales'].shift(lag)\n",
    "\n",
    "for window in [7, 14, 28]:\n",
    "    df[f'sales_roll_mean_{window}'] = (\n",
    "        df.groupby(['store_nbr', 'family_encoded'])['sales']\n",
    "        .transform(lambda x: x.shift(1).rolling(window, min_periods=1).mean())\n",
    "    )\n",
    "    df[f'sales_roll_std_{window}'] = (\n",
    "        df.groupby(['store_nbr', 'family_encoded'])['sales']\n",
    "        .transform(lambda x: x.shift(1).rolling(window, min_periods=1).std())\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Lag features created: sales_lag_7, sales_lag_14, sales_lag_28\")\n",
    "print(f\"âœ… Rolling features created: mean and std for windows 7, 14, 28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6893c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Drop rows with NaN from lag features ----------\n",
    "print(f\"\\nBefore dropping NaN: {df.shape}\")\n",
    "df_model = df.dropna().copy()\n",
    "print(f\"After dropping NaN:  {df_model.shape}\")\n",
    "print(f\"Dropped: {len(df) - len(df_model):,} rows ({(len(df)-len(df_model))/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Final feature columns\n",
    "feature_cols = [\n",
    "    'store_nbr', 'family_encoded', 'onpromotion',\n",
    "    'year', 'month', 'day', 'day_of_week', 'day_of_year', 'week_of_year',\n",
    "    'is_weekend', 'is_month_start', 'is_month_end', 'is_payday',\n",
    "    'is_earthquake_period', 'is_national_holiday', 'is_event',\n",
    "    'cluster', 'type_encoded', 'city_encoded', 'state_encoded',\n",
    "    'dcoilwtico',\n",
    "    'sales_lag_7', 'sales_lag_14', 'sales_lag_28',\n",
    "    'sales_roll_mean_7', 'sales_roll_mean_14', 'sales_roll_mean_28',\n",
    "    'sales_roll_std_7', 'sales_roll_std_14', 'sales_roll_std_28',\n",
    "]\n",
    "\n",
    "target = 'sales'\n",
    "\n",
    "print(f\"\\nâœ… Final feature set: {len(feature_cols)} features\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Modeling & Evaluation\n",
    "\n",
    "We will train **4 different models** and compare their performance to find the best forecasting approach.\n",
    "\n",
    "| # | Model | Description |\n",
    "|---|-------|-------------|\n",
    "| 1 | **Baseline (NaÃ¯ve)** | Predict using the last known value (lag 7) |\n",
    "| 2 | **Linear Regression** | Simple linear model |\n",
    "| 3 | **Random Forest** | Ensemble of decision trees |\n",
    "| 4 | **LightGBM** | Gradient boosting (state-of-the-art) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7537a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸŽ¯ Prepare Train/Validation Split\n",
    "# ============================================================\n",
    "# Use time-based split: last 15 days as validation (same as test period)\n",
    "val_start = df_model['date'].max() - timedelta(days=15)\n",
    "train_data = df_model[df_model['date'] < val_start]\n",
    "val_data = df_model[df_model['date'] >= val_start]\n",
    "\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data[target]\n",
    "X_val = val_data[feature_cols]\n",
    "y_val = val_data[target]\n",
    "\n",
    "print(f\"âœ… Train/Validation split (time-based)\")\n",
    "print(f\"   Train: {X_train.shape} | date < {val_start.date()}\")\n",
    "print(f\"   Val:   {X_val.shape} | date >= {val_start.date()}\")\n",
    "\n",
    "# Metrics functions\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a54542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model 1: Baseline (NaÃ¯ve - use lag 7)\n",
    "# ============================================================\n",
    "baseline_pred = X_val['sales_lag_7'].values\n",
    "baseline_pred = np.clip(baseline_pred, 0, None)\n",
    "\n",
    "results['Baseline (Lag 7)'] = {\n",
    "    'RMSLE': rmsle(y_val, baseline_pred),\n",
    "    'RMSE': rmse(y_val, baseline_pred),\n",
    "    'MAE': mae(y_val, baseline_pred)\n",
    "}\n",
    "print(f\"âœ… Model 1: Baseline\")\n",
    "print(f\"   RMSLE: {results['Baseline (Lag 7)']['RMSLE']:.4f}\")\n",
    "print(f\"   RMSE:  {results['Baseline (Lag 7)']['RMSE']:.2f}\")\n",
    "print(f\"   MAE:   {results['Baseline (Lag 7)']['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab07435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model 2: Linear Regression\n",
    "# ============================================================\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = np.clip(lr_model.predict(X_val), 0, None)\n",
    "\n",
    "results['Linear Regression'] = {\n",
    "    'RMSLE': rmsle(y_val, lr_pred),\n",
    "    'RMSE': rmse(y_val, lr_pred),\n",
    "    'MAE': mae(y_val, lr_pred)\n",
    "}\n",
    "print(f\"âœ… Model 2: Linear Regression\")\n",
    "print(f\"   RMSLE: {results['Linear Regression']['RMSLE']:.4f}\")\n",
    "print(f\"   RMSE:  {results['Linear Regression']['RMSE']:.2f}\")\n",
    "print(f\"   MAE:   {results['Linear Regression']['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b505cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model 3: Random Forest (sampled for memory/speed)\n",
    "# ============================================================\n",
    "# Sample training data for Random Forest (3M+ rows is too large)\n",
    "sample_frac = min(500_000 / len(X_train), 1.0)\n",
    "X_train_sample = X_train.sample(frac=sample_frac, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, max_depth=15, min_samples_leaf=10,\n",
    "    n_jobs=-1, random_state=42, verbose=0\n",
    ")\n",
    "rf_model.fit(X_train_sample, y_train_sample)\n",
    "rf_pred = np.clip(rf_model.predict(X_val), 0, None)\n",
    "\n",
    "results['Random Forest'] = {\n",
    "    'RMSLE': rmsle(y_val, rf_pred),\n",
    "    'RMSE': rmse(y_val, rf_pred),\n",
    "    'MAE': mae(y_val, rf_pred)\n",
    "}\n",
    "print(f\"âœ… Model 3: Random Forest (trained on {len(X_train_sample):,} samples)\")\n",
    "print(f\"   RMSLE: {results['Random Forest']['RMSLE']:.4f}\")\n",
    "print(f\"   RMSE:  {results['Random Forest']['RMSE']:.2f}\")\n",
    "print(f\"   MAE:   {results['Random Forest']['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258194cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model 4: LightGBM (handles large data efficiently)\n",
    "# ============================================================\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=500, max_depth=10, learning_rate=0.05,\n",
    "    num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "    min_child_samples=50, reg_alpha=0.1, reg_lambda=0.1,\n",
    "    n_jobs=-1, random_state=42, verbose=-1\n",
    ")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.log_evaluation(period=0)]\n",
    ")\n",
    "lgb_pred = np.clip(lgb_model.predict(X_val), 0, None)\n",
    "\n",
    "results['LightGBM'] = {\n",
    "    'RMSLE': rmsle(y_val, lgb_pred),\n",
    "    'RMSE': rmse(y_val, lgb_pred),\n",
    "    'MAE': mae(y_val, lgb_pred)\n",
    "}\n",
    "print(f\"âœ… Model 4: LightGBM\")\n",
    "print(f\"   RMSLE: {results['LightGBM']['RMSLE']:.4f}\")\n",
    "print(f\"   RMSE:  {results['LightGBM']['RMSE']:.2f}\")\n",
    "print(f\"   MAE:   {results['LightGBM']['MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62b0d3",
   "metadata": {},
   "source": [
    "### ðŸ“Š Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š Model Comparison\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('RMSLE')\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š MODEL COMPARISON (sorted by RMSLE)\")\n",
    "print(\"=\" * 70)\n",
    "display(results_df)\n",
    "\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} (RMSLE: {results_df.iloc[0]['RMSLE']:.4f})\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "metrics = ['RMSLE', 'RMSE', 'MAE']\n",
    "colors = [COLORS[0], COLORS[2], COLORS[4], COLORS[6]]\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    vals = results_df[metric]\n",
    "    bars = ax.bar(range(len(vals)), vals.values, color=colors[:len(vals)],\n",
    "                  edgecolor='black', alpha=0.85)\n",
    "    ax.set_title(f'ðŸ“Š {metric}', fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(range(len(vals)))\n",
    "    ax.set_xticklabels(vals.index, rotation=30, ha='right', fontsize=9)\n",
    "    for bar, val in zip(bars, vals.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val + val*0.02,\n",
    "               f'{val:.4f}' if metric == 'RMSLE' else f'{val:.1f}',\n",
    "               ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660eb104",
   "metadata": {},
   "source": [
    "### ðŸ” Feature Importance (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from LightGBM\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top20 = importance.head(20)\n",
    "ax.barh(range(len(top20)), top20['importance'].values, color=COLORS[4], edgecolor='black', alpha=0.85)\n",
    "ax.set_yticks(range(len(top20)))\n",
    "ax.set_yticklabels(top20['feature'].values)\n",
    "ax.set_title('ðŸ” Top 20 Most Important Features (LightGBM)', fontweight='bold', fontsize=16)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Top 10 Features:\")\n",
    "for i, row in importance.head(10).iterrows():\n",
    "    print(f\"   {row['feature']:30s} â†’ {row['importance']:>8.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de46077",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Best Model & Final Predictions\n",
    "\n",
    "Now we use our best-performing model to generate predictions on the **test dataset** and prepare a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ† Generate Test Predictions\n",
    "# ============================================================\n",
    "\n",
    "# Prepare test data with same features\n",
    "df_test = test.copy()\n",
    "\n",
    "# Date features\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "df_test['month'] = df_test['date'].dt.month\n",
    "df_test['day'] = df_test['date'].dt.day\n",
    "df_test['day_of_week'] = df_test['date'].dt.dayofweek\n",
    "df_test['day_of_year'] = df_test['date'].dt.dayofyear\n",
    "df_test['week_of_year'] = df_test['date'].dt.isocalendar().week.astype(int)\n",
    "df_test['is_weekend'] = (df_test['day_of_week'] >= 5).astype(int)\n",
    "df_test['is_month_start'] = df_test['date'].dt.is_month_start.astype(int)\n",
    "df_test['is_month_end'] = df_test['date'].dt.is_month_end.astype(int)\n",
    "df_test['is_payday'] = ((df_test['day'] == 15) | df_test['date'].dt.is_month_end).astype(int)\n",
    "df_test['is_earthquake_period'] = 0\n",
    "\n",
    "# Merge store info\n",
    "df_test = df_test.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "# Merge oil\n",
    "df_test = df_test.merge(oil, on='date', how='left')\n",
    "df_test['dcoilwtico'] = df_test['dcoilwtico'].interpolate().bfill().ffill()\n",
    "\n",
    "# Holiday features\n",
    "df_test['is_national_holiday'] = df_test['date'].isin(nat_holidays['date'].unique()).astype(int)\n",
    "df_test['is_event'] = df_test['date'].isin(events['date'].unique()).astype(int)\n",
    "\n",
    "# Encode categoricals (using same encoders)\n",
    "df_test['family_encoded'] = le_family.transform(df_test['family'])\n",
    "df_test['city_encoded'] = le_city.transform(df_test['city'])\n",
    "df_test['state_encoded'] = le_state.transform(df_test['state'])\n",
    "df_test['type_encoded'] = le_type.transform(df_test['type'])\n",
    "\n",
    "# Lag features from training data (last known values)\n",
    "last_known = df.groupby(['store_nbr', 'family_encoded']).tail(28).copy()\n",
    "for lag in [7, 14, 28]:\n",
    "    lag_vals = last_known.groupby(['store_nbr', 'family_encoded'])['sales'].apply(\n",
    "        lambda x: x.iloc[-lag] if len(x) >= lag else x.mean()\n",
    "    ).reset_index()\n",
    "    lag_vals.columns = ['store_nbr', 'family_encoded', f'sales_lag_{lag}']\n",
    "    df_test = df_test.merge(lag_vals, on=['store_nbr', 'family_encoded'], how='left')\n",
    "\n",
    "# Rolling features from training data\n",
    "for window in [7, 14, 28]:\n",
    "    roll_vals = last_known.groupby(['store_nbr', 'family_encoded'])['sales'].apply(\n",
    "        lambda x: x.tail(window).mean()\n",
    "    ).reset_index()\n",
    "    roll_vals.columns = ['store_nbr', 'family_encoded', f'sales_roll_mean_{window}']\n",
    "    df_test = df_test.merge(roll_vals, on=['store_nbr', 'family_encoded'], how='left')\n",
    "\n",
    "    roll_std_vals = last_known.groupby(['store_nbr', 'family_encoded'])['sales'].apply(\n",
    "        lambda x: x.tail(window).std()\n",
    "    ).reset_index()\n",
    "    roll_std_vals.columns = ['store_nbr', 'family_encoded', f'sales_roll_std_{window}']\n",
    "    df_test = df_test.merge(roll_std_vals, on=['store_nbr', 'family_encoded'], how='left')\n",
    "\n",
    "# Fill any remaining NaN\n",
    "df_test[feature_cols] = df_test[feature_cols].fillna(0)\n",
    "\n",
    "print(f\"âœ… Test features prepared: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ”® Generate Predictions with Best Model (LightGBM)\n",
    "# ============================================================\n",
    "X_test_final = df_test[feature_cols]\n",
    "test_predictions = np.clip(lgb_model.predict(X_test_final), 0, None)\n",
    "\n",
    "# Create submission\n",
    "submission_final = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'sales': test_predictions\n",
    "})\n",
    "\n",
    "# Save\n",
    "submission_final.to_csv('submission.csv', index=False)\n",
    "print(f\"âœ… Submission file saved: submission.csv\")\n",
    "print(f\"   Shape: {submission_final.shape}\")\n",
    "display(submission_final.head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“Š Prediction Summary:\")\n",
    "print(f\"   Min:    {test_predictions.min():.2f}\")\n",
    "print(f\"   Mean:   {test_predictions.mean():.2f}\")\n",
    "print(f\"   Median: {np.median(test_predictions):.2f}\")\n",
    "print(f\"   Max:    {test_predictions.max():.2f}\")\n",
    "print(f\"   Zeros:  {(test_predictions == 0).sum()} ({(test_predictions == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“ˆ Visualize Predictions\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distribution of predictions\n",
    "axes[0].hist(test_predictions[test_predictions > 0], bins=100,\n",
    "            color=COLORS[0], edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('ðŸ“Š Distribution of Predicted Sales (>0)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Predicted Sales')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Predictions over time (total daily)\n",
    "pred_daily = df_test.copy()\n",
    "pred_daily['predicted_sales'] = test_predictions\n",
    "daily_pred_total = pred_daily.groupby('date')['predicted_sales'].sum()\n",
    "daily_pred_total.plot(ax=axes[1], color=COLORS[4], linewidth=2, marker='o', markersize=5)\n",
    "axes[1].set_title('ðŸ“ˆ Total Predicted Daily Sales (Test Period)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Total Predicted Sales')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93321bb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Conclusion\n",
    "\n",
    "### What We Did:\n",
    "1. **Loaded & explored** 7 datasets covering store sales in Ecuador (2013â€“2017)\n",
    "2. **Deep EDA** revealed key drivers: product family, promotions, payday cycles, seasonality\n",
    "3. **Feature engineering** created 30+ features from dates, stores, oil prices, holidays\n",
    "4. **Model comparison** of 4 approaches showed **LightGBM** performed best\n",
    "5. **Generated predictions** for the 15-day test period\n",
    "\n",
    "### Key Takeaways for the Business:\n",
    "- ðŸ“¦ **GROCERY I and BEVERAGES** dominate sales â€” keep these well-stocked!\n",
    "- ðŸŽ¯ **Promotions work** â€” items on promotion sell significantly more\n",
    "- ðŸ’° **Pay days matter** â€” plan inventory for the 15th and end of month\n",
    "- ðŸ“… **Sunday is king** â€” highest sales day of the week\n",
    "- ðŸŽ„ **December peaks** â€” Christmas shopping drives yearly highs\n",
    "- ðŸ›¢ï¸ Oil prices affect the economy indirectly but aren't a strong direct predictor\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "### ðŸ™ Thank You for Reading!\n",
    "\n",
    "**Connect with me on [LinkedIn](https://www.linkedin.com/in/sajjad-ali-shah47/)**\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (llm)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
