{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d8763a",
   "metadata": {},
   "source": [
    "# ü•¨ Kalimati Tarkari Dataset: Fruits & Vegetables Price Analysis\n",
    "\n",
    "## üìä About Dataset\n",
    "\n",
    "### **Kalimati Tarkari Dataset**\n",
    "This comprehensive dataset contains historical price data for fruits and vegetables from the **Kalimati Fruits and Vegetable Market Development Board**, Nepal's largest wholesale market for agricultural produce. The data has been meticulously scraped from the official website: [https://kalimatimarket.gov.np/](https://kalimatimarket.gov.np/)\n",
    "\n",
    "The dataset captures daily minimum, maximum, and average prices for a wide variety of commodities, providing valuable insights into market trends, seasonal variations, and price dynamics in Nepal's agricultural sector.\n",
    "\n",
    "üì¶ **Dataset Source**: [Kaggle - Kalimati Tarkari Dataset](https://www.kaggle.com/datasets/nischallal/kalimati-tarkari-dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Context\n",
    "\n",
    "The Kalimati Market serves as the primary wholesale hub for fruits and vegetables in Nepal, directly influencing retail prices across the country. Understanding price patterns in this market is crucial for:\n",
    "\n",
    "- **Farmers** seeking optimal selling times for their produce\n",
    "- **Retailers** planning inventory and pricing strategies  \n",
    "- **Policymakers** monitoring food security and inflation\n",
    "- **Consumers** understanding seasonal price fluctuations\n",
    "- **Researchers** analyzing agricultural economics and supply chain dynamics\n",
    "\n",
    "This dataset represents years of daily price records, capturing the pulse of Nepal's agricultural market and offering a window into the economic realities of food distribution in South Asia.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Content\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "- **280,000+ records** spanning multiple years of daily price data\n",
    "- **Multiple commodity categories**: Vegetables (tomatoes, potatoes, leafy greens, etc.), Fruits (bananas, mangoes, apples, etc.), and specialty items\n",
    "- **Price metrics**: Minimum, Maximum, and Average prices per unit (Kg/Dozen/Piece)\n",
    "- **Temporal data**: Date-wise records enabling time series analysis\n",
    "- **Unit specifications**: Clear measurement units for each commodity\n",
    "\n",
    "### Key Features:\n",
    "- Daily price updates for 70+ different commodities\n",
    "- Seasonal variation patterns across different produce types\n",
    "- Price volatility indicators through min-max spreads\n",
    "- Historical trends for forecasting and predictive modeling\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Acknowledgements\n",
    "\n",
    "We extend our sincere gratitude to:\n",
    "\n",
    "- **Kalimati Fruits and Vegetable Market Development Board** for maintaining transparent and accessible price records\n",
    "- The **Government of Nepal** for supporting agricultural market information systems\n",
    "- **Open data initiatives** that make agricultural market data publicly available for research and analysis\n",
    "\n",
    "This dataset would not be possible without the continuous efforts of market officials who diligently record and publish daily price information, contributing to market transparency and informed decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Inspiration & Research Questions\n",
    "\n",
    "This dataset opens doors to numerous analytical opportunities:\n",
    "\n",
    "### üìà Time Series Analysis:\n",
    "- Can we predict future prices based on historical trends?\n",
    "- What are the seasonal patterns for different commodities?\n",
    "- How do prices fluctuate during festivals and special occasions?\n",
    "\n",
    "### üîç Market Insights:\n",
    "- Which commodities show the highest price volatility?\n",
    "- How do local vs. imported produce prices compare?\n",
    "- What is the relationship between minimum and maximum prices?\n",
    "\n",
    "### üåæ Economic Analysis:\n",
    "- How do weather patterns affect vegetable prices?\n",
    "- What is the impact of supply chain disruptions on prices?\n",
    "- Can we identify inflationary trends in food prices?\n",
    "\n",
    "### ü§ñ Machine Learning Applications:\n",
    "- Price forecasting models for different commodities\n",
    "- Anomaly detection in price patterns\n",
    "- Clustering analysis of similar price behaviors\n",
    "\n",
    "---\n",
    "\n",
    "**Let's explore the data and uncover the stories hidden in Nepal's agricultural market!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üë§ Author\n",
    "\n",
    "**Sajjad Ali Shah**  \n",
    "Data Scientist | Machine Learning Engineer  \n",
    "üîó [LinkedIn Profile](https://www.linkedin.com/in/sajjad-ali-shah47/)\n",
    "\n",
    "*Feel free to connect for collaborations, discussions, or questions about this analysis!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e887a",
   "metadata": {},
   "source": [
    "## üîó Dataset Source\n",
    "\n",
    "üì¶ **Kaggle**: [Kalimati Tarkari Dataset](https://www.kaggle.com/datasets/nischallal/kalimati-tarkari-dataset)\n",
    "\n",
    "Access the complete dataset on Kaggle for your analysis and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19b7e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ Time Series Analysis Workflow\n",
    "\n",
    "Following a systematic approach for time series analysis:\n",
    "1. üì• **Data Collection** - Load the dataset\n",
    "2. üìÖ **Datetime Handling** - Convert and parse date columns\n",
    "3. üîç **Initial Data Inspection** - Understand data structure\n",
    "4. ‚ùì **Missing Values Check** - Identify temporal gaps\n",
    "5. üìä **Resampling** (if needed) - Standardize time intervals\n",
    "6. üìà **Exploratory Data Analysis** - Discover patterns and trends\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1Ô∏è‚É£: Data Collection\n",
    "\n",
    "Loading the Kalimati Tarkari dataset for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06eb3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f57aafc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Shape: 280,862 rows, 6 columns\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df=pd.read_csv('Dataset/Kalimati_Tarkari_Dataset.csv', low_memory=False)\n",
    "\n",
    "# Convert price columns to numeric, handling any non-numeric values\n",
    "df['Minimum'] = pd.to_numeric(df['Minimum'], errors='coerce')\n",
    "df['Maximum'] = pd.to_numeric(df['Maximum'], errors='coerce')\n",
    "df['Average'] = pd.to_numeric(df['Average'], errors='coerce')\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4363d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2Ô∏è‚É£: Datetime Handling\n",
    "\n",
    "Converting date columns to proper datetime format and extracting temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3c8611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìÖ DATETIME HANDLING\n",
      "====================================================================================================\n",
      "\n",
      "üîç Original Date Column:\n",
      "Data Type: object\n",
      "Sample values:\n",
      "0    6/16/2013\n",
      "1    6/16/2013\n",
      "2    6/16/2013\n",
      "3    6/16/2013\n",
      "4    6/16/2013\n",
      "Name: Date, dtype: object\n",
      "\n",
      "‚úÖ After Conversion:\n",
      "Data Type: datetime64[ns]\n",
      "Sample values:\n",
      "0   2013-06-16\n",
      "1   2013-06-16\n",
      "2   2013-06-16\n",
      "3   2013-06-16\n",
      "4   2013-06-16\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "\n",
      "üìä Date Range:\n",
      "Start Date: 2013-06-16\n",
      "End Date: 2023-09-28\n",
      "Total Days: 3,756\n",
      "Total Years: 10.28\n",
      "\n",
      "‚úÖ New Temporal Features Created:\n",
      "   ‚Ä¢ Year, Month, Month_Name, Day\n",
      "   ‚Ä¢ Day_of_Week, Quarter, Week_of_Year\n"
     ]
    }
   ],
   "source": [
    "# Convert Date column to datetime format\n",
    "print(\"=\" * 100)\n",
    "print(\"üìÖ DATETIME HANDLING\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check original date format\n",
    "print(f\"\\nüîç Original Date Column:\")\n",
    "print(f\"Data Type: {df['Date'].dtype}\")\n",
    "print(f\"Sample values:\\n{df['Date'].head()}\")\n",
    "\n",
    "# Convert to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='mixed', dayfirst=False)\n",
    "\n",
    "print(f\"\\n‚úÖ After Conversion:\")\n",
    "print(f\"Data Type: {df['Date'].dtype}\")\n",
    "print(f\"Sample values:\\n{df['Date'].head()}\")\n",
    "\n",
    "# Extract temporal features\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Month_Name'] = df['Date'].dt.month_name()\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Day_of_Week'] = df['Date'].dt.day_name()\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "\n",
    "print(f\"\\nüìä Date Range:\")\n",
    "print(f\"Start Date: {df['Date'].min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"End Date: {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total Days: {(df['Date'].max() - df['Date'].min()).days:,}\")\n",
    "print(f\"Total Years: {(df['Date'].max() - df['Date'].min()).days / 365.25:.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ New Temporal Features Created:\")\n",
    "print(f\"   ‚Ä¢ Year, Month, Month_Name, Day\")\n",
    "print(f\"   ‚Ä¢ Day_of_Week, Quarter, Week_of_Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c8a77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3Ô∏è‚É£: Initial Data Inspection\n",
    "\n",
    "Understanding the structure and basic characteristics of the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8840cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìä DATASET INFORMATION\n",
      "====================================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 280862 entries, 0 to 280861\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Commodity     280862 non-null  object        \n",
      " 1   Date          280862 non-null  datetime64[ns]\n",
      " 2   Unit          280862 non-null  object        \n",
      " 3   Minimum       229706 non-null  float64       \n",
      " 4   Maximum       229706 non-null  float64       \n",
      " 5   Average       229706 non-null  float64       \n",
      " 6   Year          280862 non-null  int32         \n",
      " 7   Month         280862 non-null  int32         \n",
      " 8   Month_Name    280862 non-null  object        \n",
      " 9   Day           280862 non-null  int32         \n",
      " 10  Day_of_Week   280862 non-null  object        \n",
      " 11  Quarter       280862 non-null  int32         \n",
      " 12  Week_of_Year  280862 non-null  UInt32        \n",
      "dtypes: UInt32(1), datetime64[ns](1), float64(3), int32(4), object(4)\n",
      "memory usage: 22.8+ MB\n",
      "\n",
      "====================================================================================================\n",
      "üìê DATASET SHAPE\n",
      "====================================================================================================\n",
      "Rows: 280,862\n",
      "Columns: 13\n",
      "\n",
      "====================================================================================================\n",
      "üîç FIRST 5 ROWS\n",
      "====================================================================================================\n",
      "             Commodity       Date Unit  Minimum  Maximum  Average  Year  \\\n",
      "0   Tomato Big(Nepali) 2013-06-16   Kg     35.0     40.0     37.5  2013   \n",
      "1  Tomato Small(Local) 2013-06-16   Kg     26.0     32.0     29.0  2013   \n",
      "2           Potato Red 2013-06-16   Kg     20.0     21.0     20.5  2013   \n",
      "3         Potato White 2013-06-16   Kg     15.0     16.0     15.5  2013   \n",
      "4   Onion Dry (Indian) 2013-06-16   Kg     28.0     30.0     29.0  2013   \n",
      "\n",
      "   Month Month_Name  Day Day_of_Week  Quarter  Week_of_Year  \n",
      "0      6       June   16      Sunday        2            24  \n",
      "1      6       June   16      Sunday        2            24  \n",
      "2      6       June   16      Sunday        2            24  \n",
      "3      6       June   16      Sunday        2            24  \n",
      "4      6       June   16      Sunday        2            24  \n",
      "\n",
      "====================================================================================================\n",
      "üìà STATISTICAL SUMMARY\n",
      "====================================================================================================\n",
      "                                Date        Minimum        Maximum  \\\n",
      "count                         280862  229706.000000  229706.000000   \n",
      "mean   2019-02-01 03:35:45.449366528      90.336273     100.225105   \n",
      "min              2013-06-16 00:00:00       1.000000       6.000000   \n",
      "25%              2016-06-17 00:00:00      40.000000      45.000000   \n",
      "50%              2019-05-23 00:00:00      60.000000      70.000000   \n",
      "75%              2021-10-15 00:00:00     100.000000     120.000000   \n",
      "max              2023-09-28 00:00:00    2800.000000    3000.000000   \n",
      "std                              NaN      86.931288      94.916482   \n",
      "\n",
      "             Average           Year          Month            Day  \\\n",
      "count  229706.000000  280862.000000  280862.000000  280862.000000   \n",
      "mean       95.280689    2018.589788       6.481201      15.753509   \n",
      "min         5.000000    2013.000000       1.000000       1.000000   \n",
      "25%        42.500000    2016.000000       3.000000       8.000000   \n",
      "50%        65.000000    2019.000000       7.000000      16.000000   \n",
      "75%       110.000000    2021.000000       9.000000      23.000000   \n",
      "max      2900.000000    2023.000000      12.000000      31.000000   \n",
      "std        90.812719       3.022381       3.461631       8.786222   \n",
      "\n",
      "             Quarter  Week_of_Year  \n",
      "count  280862.000000      280862.0  \n",
      "mean        2.491793      26.43322  \n",
      "min         1.000000           1.0  \n",
      "25%         1.000000          13.0  \n",
      "50%         3.000000          26.0  \n",
      "75%         3.000000          39.0  \n",
      "max         4.000000          53.0  \n",
      "std         1.115800     15.135946  \n",
      "\n",
      "====================================================================================================\n",
      "üîé MISSING VALUES\n",
      "====================================================================================================\n",
      "Minimum    51156\n",
      "Maximum    51156\n",
      "Average    51156\n",
      "dtype: int64\n",
      "\n",
      "====================================================================================================\n",
      "üìä DUPLICATE ROWS\n",
      "====================================================================================================\n",
      "Total duplicate rows: 0\n",
      "‚úÖ No duplicate rows found!\n",
      "\n",
      "====================================================================================================\n",
      "üè∑Ô∏è UNIQUE VALUES PER COLUMN\n",
      "====================================================================================================\n",
      "Commodity: 136 unique values\n",
      "Date: 3,615 unique values\n",
      "Unit: 6 unique values\n",
      "Minimum: 208 unique values\n",
      "Maximum: 218 unique values\n",
      "Average: 406 unique values\n",
      "Year: 11 unique values\n",
      "Month: 12 unique values\n",
      "Month_Name: 12 unique values\n",
      "Day: 31 unique values\n",
      "Day_of_Week: 7 unique values\n",
      "Quarter: 4 unique values\n",
      "Week_of_Year: 53 unique values\n"
     ]
    }
   ],
   "source": [
    "# Dataset Overview\n",
    "print(\"=\" * 100)\n",
    "print(\"üìä DATASET INFORMATION\")\n",
    "print(\"=\" * 100)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìê DATASET SHAPE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üîç FIRST 5 ROWS\")\n",
    "print(\"=\" * 100)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üîé MISSING VALUES\")\n",
    "print(\"=\" * 100)\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä DUPLICATE ROWS\")\n",
    "print(\"=\" * 100)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicates:,}\")\n",
    "if duplicates == 0:\n",
    "    print(\"‚úÖ No duplicate rows found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üè∑Ô∏è UNIQUE VALUES PER COLUMN\")\n",
    "print(\"=\" * 100)\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique():,} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd236f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4Ô∏è‚É£: Missing Values in Time Series\n",
    "\n",
    "Identifying temporal gaps and missing data points in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8720be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "‚ùì MISSING VALUES IN TIME SERIES\n",
      "====================================================================================================\n",
      "\n",
      "üìä Missing Values by Column:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " Column  Missing Count  Missing %\n",
      "Minimum          51156  18.213927\n",
      "Maximum          51156  18.213927\n",
      "Average          51156  18.213927\n",
      "\n",
      "====================================================================================================\n",
      "üìÖ Temporal Continuity Check:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Expected date range: 3,757 days\n",
      "Actual unique dates: 3,615 days\n",
      "Missing dates: 142 days\n",
      "\n",
      "‚ö†Ô∏è Found 142 missing dates in the time series\n",
      "First 10 missing dates:\n",
      "  ‚Ä¢ 2013-06-22\n",
      "  ‚Ä¢ 2013-06-23\n",
      "  ‚Ä¢ 2013-06-24\n",
      "  ‚Ä¢ 2013-06-29\n",
      "  ‚Ä¢ 2013-07-06\n",
      "  ‚Ä¢ 2013-07-07\n",
      "  ‚Ä¢ 2013-07-08\n",
      "  ‚Ä¢ 2013-07-13\n",
      "  ‚Ä¢ 2013-07-20\n",
      "  ‚Ä¢ 2013-07-27\n",
      "\n",
      "====================================================================================================\n",
      "üìä Time Series Frequency Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total Records: 280,862\n",
      "Unique Commodities: 136\n",
      "Records per Commodity (average): 2065.16\n",
      "\n",
      "====================================================================================================\n",
      "üìà Data Distribution Over Time:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Year\n",
      "2013    10896\n",
      "2014    22170\n",
      "2015    25084\n",
      "2016    25596\n",
      "2017    24124\n",
      "2018    23295\n",
      "2019    26197\n",
      "2020    26622\n",
      "2021    34685\n",
      "2022    35084\n",
      "2023    27109\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Missing values analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Time Series Missing Values Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"‚ùì MISSING VALUES IN TIME SERIES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Check for missing values in columns\n",
    "print(\"\\nüìä Missing Values by Column:\")\n",
    "print(\"-\" * 100)\n",
    "missing_count = df.isnull().sum()\n",
    "missing_percent = (missing_count / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_count.index,\n",
    "    'Missing Count': missing_count.values,\n",
    "    'Missing %': missing_percent.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].to_string(index=False) if missing_df['Missing Count'].sum() > 0 else \"‚úÖ No missing values in columns!\")\n",
    "\n",
    "# 2. Check for temporal gaps (missing dates)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìÖ Temporal Continuity Check:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Sort by date\n",
    "df_sorted = df.sort_values('Date')\n",
    "\n",
    "# Get all unique dates\n",
    "unique_dates = df_sorted['Date'].unique()\n",
    "date_range = pd.date_range(start=df_sorted['Date'].min(), end=df_sorted['Date'].max(), freq='D')\n",
    "\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(unique_dates))\n",
    "\n",
    "print(f\"Expected date range: {len(date_range):,} days\")\n",
    "print(f\"Actual unique dates: {len(unique_dates):,} days\")\n",
    "print(f\"Missing dates: {len(missing_dates):,} days\")\n",
    "\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(missing_dates)} missing dates in the time series\")\n",
    "    print(f\"First 10 missing dates:\")\n",
    "    for date in missing_dates[:10]:\n",
    "        print(f\"  ‚Ä¢ {date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No temporal gaps found - continuous daily data!\")\n",
    "\n",
    "# 3. Check data frequency\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìä Time Series Frequency Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Unique Commodities: {df['Commodity'].nunique()}\")\n",
    "print(f\"Records per Commodity (average): {len(df) / df['Commodity'].nunique():.2f}\")\n",
    "\n",
    "# 4. Check for irregular spacing\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"üìà Data Distribution Over Time:\")\n",
    "print(\"-\" * 100)\n",
    "records_per_year = df.groupby('Year').size()\n",
    "print(records_per_year)\n",
    "\n",
    "print(\"\\n‚úÖ Missing values analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b20309",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5Ô∏è‚É£: Resampling (If Needed)\n",
    "\n",
    "Checking if resampling is required and standardizing time intervals if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7bf8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìä RESAMPLING ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "üîç Current Data Frequency:\n",
      "   ‚Ä¢ Data appears to be DAILY (one record per commodity per day)\n",
      "   ‚Ä¢ Multiple commodities tracked simultaneously\n",
      "\n",
      "üìà Sample Resampling for 'Cauli Local':\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "1Ô∏è‚É£ Daily Frequency (Current):\n",
      "   Records: 3612\n",
      "   Sample:\n",
      "            Average\n",
      "Date               \n",
      "2013-06-16     32.5\n",
      "2013-06-17     27.5\n",
      "2013-06-18     27.5\n",
      "2013-06-19     27.5\n",
      "2013-06-20     22.5\n",
      "\n",
      "2Ô∏è‚É£ Weekly Resampling (Mean prices):\n",
      "   Records: 538\n",
      "   Sample:\n",
      "            Minimum  Maximum  Average\n",
      "Date                                 \n",
      "2013-06-16     30.0     35.0     32.5\n",
      "2013-06-23     23.0     27.8     25.4\n",
      "2013-06-30     23.6     29.4     26.5\n",
      "2013-07-07     35.0     40.0     37.5\n",
      "2013-07-14     47.0     54.0     50.5\n",
      "\n",
      "3Ô∏è‚É£ Monthly Resampling (Mean prices):\n",
      "   Records: 124\n",
      "   Sample:\n",
      "              Minimum    Maximum    Average\n",
      "Date                                       \n",
      "2013-06-30  23.909091  29.181818  26.545455\n",
      "2013-07-31  47.600000  54.400000  51.000000\n",
      "2013-08-31  54.047619  59.857143  56.952381\n",
      "2013-09-30  40.960000  47.920000  44.440000\n",
      "2013-10-31  44.259259  52.481481  48.370370\n",
      "\n",
      "üí° Resampling Options:\n",
      "   ‚Ä¢ Daily (D) - Current frequency ‚úÖ\n",
      "   ‚Ä¢ Weekly (W) - For weekly trend analysis\n",
      "   ‚Ä¢ Monthly (M) - For long-term patterns\n",
      "   ‚Ä¢ Quarterly (Q) - For seasonal analysis\n",
      "\n",
      "üìå Decision: Keep DAILY frequency for detailed analysis\n",
      "   (Can resample later for specific visualizations or modeling)\n",
      "\n",
      "‚úÖ Resampling analysis complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sk451\\AppData\\Local\\Temp\\ipykernel_9232\\1158439283.py:30: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = commodity_data[['Minimum', 'Maximum', 'Average']].resample('M').mean()\n"
     ]
    }
   ],
   "source": [
    "# Resampling Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"üìä RESAMPLING ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check current frequency\n",
    "print(f\"\\nüîç Current Data Frequency:\")\n",
    "print(f\"   ‚Ä¢ Data appears to be DAILY (one record per commodity per day)\")\n",
    "print(f\"   ‚Ä¢ Multiple commodities tracked simultaneously\")\n",
    "\n",
    "# Example: Resample one commodity to different frequencies\n",
    "sample_commodity = df['Commodity'].value_counts().index[0]\n",
    "commodity_data = df[df['Commodity'] == sample_commodity].set_index('Date').sort_index()\n",
    "\n",
    "print(f\"\\nüìà Sample Resampling for '{sample_commodity}':\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Daily (current)\n",
    "print(f\"\\n1Ô∏è‚É£ Daily Frequency (Current):\")\n",
    "print(f\"   Records: {len(commodity_data)}\")\n",
    "print(f\"   Sample:\\n{commodity_data[['Average']].head()}\")\n",
    "\n",
    "# Weekly resampling\n",
    "weekly = commodity_data[['Minimum', 'Maximum', 'Average']].resample('W').mean()\n",
    "print(f\"\\n2Ô∏è‚É£ Weekly Resampling (Mean prices):\")\n",
    "print(f\"   Records: {len(weekly)}\")\n",
    "print(f\"   Sample:\\n{weekly.head()}\")\n",
    "\n",
    "# Monthly resampling\n",
    "monthly = commodity_data[['Minimum', 'Maximum', 'Average']].resample('M').mean()\n",
    "print(f\"\\n3Ô∏è‚É£ Monthly Resampling (Mean prices):\")\n",
    "print(f\"   Records: {len(monthly)}\")\n",
    "print(f\"   Sample:\\n{monthly.head()}\")\n",
    "\n",
    "print(f\"\\nüí° Resampling Options:\")\n",
    "print(f\"   ‚Ä¢ Daily (D) - Current frequency ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Weekly (W) - For weekly trend analysis\")\n",
    "print(f\"   ‚Ä¢ Monthly (M) - For long-term patterns\")\n",
    "print(f\"   ‚Ä¢ Quarterly (Q) - For seasonal analysis\")\n",
    "\n",
    "print(f\"\\nüìå Decision: Keep DAILY frequency for detailed analysis\")\n",
    "print(f\"   (Can resample later for specific visualizations or modeling)\")\n",
    "\n",
    "print(\"\\n‚úÖ Resampling analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e8acc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6Ô∏è‚É£: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now that we have a clean, well-structured time series dataset, let's explore patterns, trends, and insights through comprehensive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be033209",
   "metadata": {},
   "source": [
    "### üìÖ Temporal Distribution Analysis\n",
    "Understanding how data is distributed across time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c85b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìÖ TEMPORAL DISTRIBUTION ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "üìÜ Date Range Summary:\n",
      "   Start Date: 2013-06-16\n",
      "   End Date: 2023-09-28\n",
      "   Total Days: 3,756\n",
      "   Total Years: 10.28\n",
      "\n",
      "üìä Records Per Year:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2013:  10,896 records\n",
      "2014:  22,170 records\n",
      "2015:  25,084 records\n",
      "2016:  25,596 records\n",
      "2017:  24,124 records\n",
      "2018:  23,295 records\n",
      "2019:  26,197 records\n",
      "2020:  26,622 records\n",
      "2021:  34,685 records\n",
      "2022:  35,084 records\n",
      "2023:  27,109 records\n",
      "\n",
      "üìä Records Per Month:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "January     :  24,715 records\n",
      "February    :  22,520 records\n",
      "March       :  23,965 records\n",
      "April       :  22,909 records\n",
      "May         :  22,651 records\n",
      "June        :  23,315 records\n",
      "July        :  24,565 records\n",
      "August      :  24,159 records\n",
      "September   :  23,524 records\n",
      "October     :  21,391 records\n",
      "November    :  22,737 records\n",
      "December    :  24,411 records\n",
      "\n",
      "üìä Records Per Day of Week:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Monday      :  40,949 records\n",
      "Tuesday     :  40,915 records\n",
      "Wednesday   :  40,930 records\n",
      "Thursday    :  40,537 records\n",
      "Friday      :  40,631 records\n",
      "Saturday    :  36,470 records\n",
      "Sunday      :  40,430 records\n",
      "\n",
      "‚úÖ Temporal distribution analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Temporal Distribution Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"üìÖ TEMPORAL DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nüìÜ Date Range Summary:\")\n",
    "print(f\"   Start Date: {df['Date'].min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   End Date: {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Total Days: {(df['Date'].max() - df['Date'].min()).days:,}\")\n",
    "print(f\"   Total Years: {(df['Date'].max() - df['Date'].min()).days / 365.25:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Records Per Year:\")\n",
    "print(\"-\" * 100)\n",
    "year_counts = df['Year'].value_counts().sort_index()\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"{year}: {count:7,} records\")\n",
    "\n",
    "print(f\"\\nüìä Records Per Month:\")\n",
    "print(\"-\" * 100)\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "month_counts = df['Month_Name'].value_counts().reindex(month_order)\n",
    "for month, count in month_counts.items():\n",
    "    print(f\"{month:12}: {count:7,} records\")\n",
    "\n",
    "print(f\"\\nüìä Records Per Day of Week:\")\n",
    "print(\"-\" * 100)\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = df['Day_of_Week'].value_counts().reindex(day_order)\n",
    "for day, count in day_counts.items():\n",
    "    print(f\"{day:12}: {count:7,} records\")\n",
    "\n",
    "print(\"\\n‚úÖ Temporal distribution analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84514dcc",
   "metadata": {},
   "source": [
    "### ü•¨ Commodity Analysis\n",
    "Exploring the different commodities and their frequency in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626678f0",
   "metadata": {},
   "source": [
    "### üí∞ Price Analysis\n",
    "Analyzing price distributions, volatility, and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2900334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "ü•¨ COMMODITY ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "üìä Total Unique Commodities: 136\n",
      "\n",
      "üìà Top 20 Most Frequently Recorded Commodities:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " 1. Cauli Local                    -  3,612 records\n",
      " 2. Ginger                         -  3,612 records\n",
      " 3. Chilli Dry                     -  3,609 records\n",
      " 4. Banana                         -  3,604 records\n",
      " 5. Coriander Green                -  3,603 records\n",
      " 6. Bamboo Shoot                   -  3,603 records\n",
      " 7. Potato Red                     -  3,602 records\n",
      " 8. Brd Leaf Mustard               -  3,602 records\n",
      " 9. French Bean(Local)             -  3,600 records\n",
      "10. Cabbage(Local)                 -  3,600 records\n",
      "11. Carrot(Local)                  -  3,596 records\n",
      "12. Onion Green                    -  3,593 records\n",
      "13. Chilli Green                   -  3,592 records\n",
      "14. Garlic Dry Chinese             -  3,589 records\n",
      "15. Raddish White(Local)           -  3,588 records\n",
      "16. Brinjal Long                   -  3,586 records\n",
      "17. Lime                           -  3,585 records\n",
      "18. Mushroom(Kanya)                -  3,584 records\n",
      "19. Onion Dry (Indian)             -  3,582 records\n",
      "20. Tomato Small(Local)            -  3,581 records\n",
      "\n",
      "üìä Unit Types Distribution:\n",
      "Unit\n",
      "Kg           212344\n",
      "KG            60732\n",
      "1 Pc           4056\n",
      "Doz            3080\n",
      "Per Dozen       524\n",
      "Per Piece       126\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üí° Sample Commodities by Unit Type:\n",
      "\n",
      "Kg:\n",
      "  ‚Ä¢ Tomato Big(Nepali)\n",
      "  ‚Ä¢ Tomato Small(Local)\n",
      "  ‚Ä¢ Potato Red\n",
      "  ‚Ä¢ Potato White\n",
      "  ‚Ä¢ Onion Dry (Indian)\n",
      "\n",
      "Doz:\n",
      "  ‚Ä¢ Banana\n",
      "\n",
      "1 Pc:\n",
      "  ‚Ä¢ Pineapple\n",
      "  ‚Ä¢ Sugarcane\n",
      "  ‚Ä¢ Maize\n",
      "\n",
      "KG:\n",
      "  ‚Ä¢ Potato Red(Indian)\n",
      "  ‚Ä¢ Mushroom(Button)\n",
      "  ‚Ä¢ Apple(Fuji)\n",
      "  ‚Ä¢ Cucumber(Hybrid)\n",
      "  ‚Ä¢ Chilli Green(Bullet)\n",
      "\n",
      "Per Dozen:\n",
      "  ‚Ä¢ Banana\n",
      "\n",
      "Per Piece:\n",
      "  ‚Ä¢ Sugarcane\n"
     ]
    }
   ],
   "source": [
    "# Commodity Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"ü•¨ COMMODITY ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\nüìä Total Unique Commodities: {df['Commodity'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìà Top 20 Most Frequently Recorded Commodities:\")\n",
    "print(\"-\" * 100)\n",
    "top_commodities = df['Commodity'].value_counts().head(20)\n",
    "for idx, (commodity, count) in enumerate(top_commodities.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - {count:6,} records\")\n",
    "\n",
    "print(f\"\\nüìä Unit Types Distribution:\")\n",
    "print(df['Unit'].value_counts())\n",
    "\n",
    "print(f\"\\nüí° Sample Commodities by Unit Type:\")\n",
    "for unit in df['Unit'].unique():\n",
    "    commodities = df[df['Unit'] == unit]['Commodity'].unique()[:5]\n",
    "    print(f\"\\n{unit}:\")\n",
    "    for commodity in commodities:\n",
    "        print(f\"  ‚Ä¢ {commodity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead865f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üí∞ PRICE ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "üìä Overall Price Statistics:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Average Minimum Price: ‚Ç® 90.34\n",
      "Average Maximum Price: ‚Ç® 100.23\n",
      "Average Price: ‚Ç® 95.28\n",
      "\n",
      "üìà Price Range Statistics:\n",
      "Average Price Range: ‚Ç® 9.89\n",
      "Median Price Range: ‚Ç® 10.00\n",
      "Max Price Range: ‚Ç® 590.00\n",
      "\n",
      "üìä Top 10 Commodities by Average Price:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " 1. Strawberry                     - ‚Ç® 467.36\n",
      " 2. Asparagus                      - ‚Ç® 436.77\n",
      " 3. Kiwi                           - ‚Ç® 372.61\n",
      " 4. Avocado                        - ‚Ç® 371.75\n",
      " 5. Mushroom(Button)               - ‚Ç® 349.11\n",
      " 6. Lime                           - ‚Ç® 322.50\n",
      " 7. Grapes(Black)                  - ‚Ç® 286.64\n",
      " 8. Fish Fresh(Rahu)               - ‚Ç® 284.43\n",
      " 9. Apple(Fuji)                    - ‚Ç® 271.49\n",
      "10. Chilli Dry                     - ‚Ç® 258.29\n",
      "\n",
      "üìä Top 10 Most Volatile Commodities (by price range):\n",
      "----------------------------------------------------------------------------------------------------\n",
      " 1. Cabbage(Terai)                 - 23.70%\n",
      " 2. Cucumber(Hybrid)               - 23.26%\n",
      " 3. Cabbage                        - 22.37%\n",
      " 4. Raddish White(Local)           - 22.34%\n",
      " 5. Raddish White(Hybrid)          - 21.97%\n",
      " 6. Cauli Terai                    - 21.78%\n",
      " 7. Christophine                   - 21.02%\n",
      " 8. Tomato Small(Terai)            - 20.97%\n",
      " 9. Cabbage(Local)                 - 20.27%\n",
      "10. Tomato Small(Tunnel)           - 19.92%\n"
     ]
    }
   ],
   "source": [
    "# Price Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"üí∞ PRICE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Overall price statistics\n",
    "print(f\"\\nüìä Overall Price Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Average Minimum Price: ‚Ç® {df['Minimum'].mean():.2f}\")\n",
    "print(f\"Average Maximum Price: ‚Ç® {df['Maximum'].mean():.2f}\")\n",
    "print(f\"Average Price: ‚Ç® {df['Average'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nüìà Price Range Statistics:\")\n",
    "df['Price_Range'] = df['Maximum'] - df['Minimum']\n",
    "print(f\"Average Price Range: ‚Ç® {df['Price_Range'].mean():.2f}\")\n",
    "print(f\"Median Price Range: ‚Ç® {df['Price_Range'].median():.2f}\")\n",
    "print(f\"Max Price Range: ‚Ç® {df['Price_Range'].max():.2f}\")\n",
    "\n",
    "# Calculate price volatility (coefficient of variation)\n",
    "df['Price_Volatility'] = (df['Price_Range'] / df['Average']) * 100\n",
    "\n",
    "print(f\"\\nüìä Top 10 Commodities by Average Price:\")\n",
    "print(\"-\" * 100)\n",
    "top_priced = df.groupby('Commodity')['Average'].mean().sort_values(ascending=False).head(10)\n",
    "for idx, (commodity, price) in enumerate(top_priced.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - ‚Ç® {price:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Top 10 Most Volatile Commodities (by price range):\")\n",
    "print(\"-\" * 100)\n",
    "most_volatile = df.groupby('Commodity')['Price_Volatility'].mean().sort_values(ascending=False).head(10)\n",
    "for idx, (commodity, volatility) in enumerate(most_volatile.items(), 1):\n",
    "    print(f\"{idx:2}. {commodity:30} - {volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa8a1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "Congratulations! You've successfully completed the 6-step Time Series Analysis workflow:\n",
    "\n",
    "‚úÖ **Step 1: Data Collection** - Loaded the Kalimati Tarkari dataset  \n",
    "‚úÖ **Step 2: Datetime Handling** - Converted dates and extracted temporal features  \n",
    "‚úÖ **Step 3: Initial Data Inspection** - Explored data structure and characteristics  \n",
    "‚úÖ **Step 4: Missing Values Check** - Identified temporal gaps and missing data  \n",
    "‚úÖ **Step 5: Resampling** - Analyzed frequency and resampling options  \n",
    "‚úÖ **Step 6: Exploratory Data Analysis** - Discovered patterns and insights  \n",
    "\n",
    "### üéØ Key Findings:\n",
    "\n",
    "- **Dataset Coverage**: 10+ years of daily price data (2013-2023)\n",
    "- **Scale**: 280,862 records across 136 commodities\n",
    "- **Temporal Coverage**: 3,615 unique dates with 142 missing dates\n",
    "- **Most Expensive**: Strawberry, Asparagus, Kiwi, Avocado\n",
    "- **Most Volatile**: Cabbage varieties and Cucumber showing highest price fluctuations\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "You can now proceed with:\n",
    "- Time series visualization (line plots, seasonal decomposition)\n",
    "- Statistical modeling (ARIMA, Prophet, etc.)\n",
    "- Price forecasting and prediction\n",
    "- Anomaly detection in price patterns\n",
    "- Correlation analysis between commodities\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Analyzing! üìä**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
